{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN implementation by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_metrics(path, metrics, epoch=None):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"d_loss\"], label=\"discriminative loss\", color=\"b\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"dloss\" + str(epoch) + \".png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"g_loss\"], label=\"generative loss\", color=\"r\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"g_loss\" + str(epoch) + \".png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"g_loss\"], label=\"generative loss\", color=\"r\")\n",
    "    plt.plot(metrics[\"d_loss\"], label=\"discriminative loss\", color=\"b\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"both_loss\" + str(epoch) + \".png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save generated images\n",
    "def save_imgs(path, images, plot_dim=(5,12), size=(24,10), epoch=None):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    examples = plot_dim[0]*plot_dim[1]\n",
    "\n",
    "    # 表示\n",
    "    fig = plt.figure(figsize=size)\n",
    "    for i in range(examples):\n",
    "        if i > images.shape[0]-1:\n",
    "            continue\n",
    "        plt.subplot(plot_dim[0], plot_dim[1], i+1)\n",
    "        img = images[i, :]\n",
    "        img = img.reshape((128, 128, 3))\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.savefig(os.path.join(path, str(epoch) + \".png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, batch_size):\n",
    "        self.reuse = False\n",
    "        self.z_dim = 128\n",
    "        self.s_size = 8\n",
    "        self.depths = [1024, 512, 256, 128, 3]\n",
    "\n",
    "        self.num_res_blocks = 16\n",
    "        self.num_pixel_CNN_blocks = 3\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __call__(self, z, training=False):\n",
    "        with tf.variable_scope('g', reuse=self.reuse):\n",
    "            # reshape from inputs\n",
    "            with tf.variable_scope('fc0'):\n",
    "                z0 = tf.reshape(z, [-1, self.z_dim])\n",
    "\n",
    "                fc0 = tf.layers.dense(z0, 64*16*16, kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))            \n",
    "                fc0 = tf.layers.batch_normalization(fc0, training=training)\n",
    "                fc0 = tf.nn.relu(fc0)\n",
    "                fc0 = tf.reshape(fc0, [-1,16,16,64])\n",
    "\n",
    "            assert fc0.get_shape().as_list()[1:] == [16,16,64]\n",
    "            \n",
    "            layers = []\n",
    "            layers.append(fc0)\n",
    "            \n",
    "            for i in range(int(self.num_res_blocks)):\n",
    "                with tf.variable_scope('res_%d' % (i+1)):\n",
    "                    res = tf.layers.conv2d(layers[-1], 64, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02)) # 8x8\n",
    "                    res = tf.layers.batch_normalization(res, training=training)\n",
    "                    res = tf.nn.relu(res)\n",
    "\n",
    "                    res = tf.layers.conv2d(res, 64, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                    res = tf.layers.batch_normalization(res, training=training)\n",
    "                    res = layers[-1] + res\n",
    "                    layers.append(res)\n",
    "            \n",
    "            assert layers[-1].get_shape().as_list()[1:] == [16,16,64]\n",
    "            \n",
    "            with tf.variable_scope('conv17'):\n",
    "                conv17 = tf.layers.conv2d(layers[-1], 64, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02)) # 8x8\n",
    "                conv17 = tf.layers.batch_normalization(conv17, training=training)\n",
    "                conv17 = tf.nn.relu(conv17)\n",
    "                conv17 = layers[0] + conv17\n",
    "                layers.append(conv17)\n",
    "\n",
    "            assert layers[-1].get_shape().as_list()[1:] == [16, 16, 64]\n",
    "\n",
    "            def pixel_shuffle_layer(x, r, n_split):\n",
    "                def PS(x, r):\n",
    "                    bs, a, b, c = x.get_shape().as_list()\n",
    "                    bs = tf.shape(x)[0]\n",
    "                    x = tf.reshape(x, (bs, a, b, r, r))\n",
    "                    x = tf.transpose(x, (0, 1, 2, 4, 3))\n",
    "                    x = tf.split(x, a, 1)\n",
    "                    x = tf.concat([tf.squeeze(x_) for x_ in x], 2)\n",
    "                    x = tf.split(x, b, 1)\n",
    "                    x = tf.concat([tf.squeeze(x_) for x_ in x], 2)\n",
    "                    return tf.reshape(x, (bs, a*r, b*r, 1))\n",
    "\n",
    "                xc = tf.split(x, n_split, 3)\n",
    "                return tf.concat([PS(x_, r) for x_ in xc], 3)\n",
    "\n",
    "            for i in range(int(self.num_pixel_CNN_blocks)):\n",
    "                with tf.variable_scope('pixel_CNN_%d' % (i+1)):\n",
    "                    ps = tf.layers.conv2d(layers[-1], 256, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02)) # 8x8\n",
    "                    #print(ps.get_shape())\n",
    "                    ps = pixel_shuffle_layer(ps, 2, 64)\n",
    "                    ps = tf.layers.batch_normalization(ps, training=training)\n",
    "                    ps = tf.nn.relu(ps)\n",
    "                    layers.append(ps)\n",
    "                    #print(ps.get_shape())\n",
    "\n",
    "            assert layers[-1].get_shape().as_list()[1:] == [128, 128, 64]\n",
    "                    \n",
    "            with tf.variable_scope('output'):\n",
    "                output = tf.layers.conv2d(layers[-1], 3, [9,9], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02)) # 8x8\n",
    "                #output = tf.nn.tanh(output)\n",
    "                output = tf.nn.sigmoid(output)\n",
    "\n",
    "            assert output.get_shape().as_list()[1:] == [128, 128, 3]\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='g')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.X_dim = 128*128*3\n",
    "        self.s_size = 8\n",
    "        self.depths = [3, 32, 128, 256, 512]\n",
    "\n",
    "    def __call__(self, x, training=False, name=''):\n",
    "        def leaky_relu(x, leak=0.2, name='outputs'):\n",
    "            return tf.maximum(x, x * leak, name=name)\n",
    "\n",
    "        with tf.name_scope('d' + name), tf.variable_scope('d', reuse=self.reuse):\n",
    "            # convolution x 4\n",
    "            x = tf.reshape(x, [-1, 128, 128, 3])\n",
    "            with tf.variable_scope('conv1'):\n",
    "                conv1 = tf.layers.conv2d(x, 32, [4, 4], [2 ,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                conv1 = leaky_relu(conv1)\n",
    "\n",
    "            with tf.variable_scope('res1'):\n",
    "                res1 = tf.layers.conv2d(conv1, 32, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res1 = leaky_relu(res1)\n",
    "                res1 = tf.layers.conv2d(res1, 32, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res1 = res1 + conv1\n",
    "                res1 = leaky_relu(res1)\n",
    "\n",
    "            with tf.variable_scope('res2'):\n",
    "                res2 = tf.layers.conv2d(res1, 32, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res2 = leaky_relu(res2)\n",
    "                res2 = tf.layers.conv2d(res2, 32, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res2 = res2 + res1\n",
    "                res2 = leaky_relu(res2)\n",
    "\n",
    "            with tf.variable_scope('conv2'):\n",
    "                conv2 = tf.layers.conv2d(res2, 64, [4, 4], [2 ,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                conv2 = leaky_relu(conv2)\n",
    "\n",
    "            with tf.variable_scope('res3'):\n",
    "                res3 = tf.layers.conv2d(conv2, 64, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res3 = leaky_relu(res3)\n",
    "                res3 = tf.layers.conv2d(res3, 64, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res3 = leaky_relu(res3)\n",
    "                res3 = res3 + conv2\n",
    "                res3 = leaky_relu(res3)\n",
    "\n",
    "            with tf.variable_scope('res4'):\n",
    "                res4 = tf.layers.conv2d(res3, 64, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res4 = leaky_relu(res4)\n",
    "                res4 = tf.layers.conv2d(res4, 64, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res4 = leaky_relu(res4)\n",
    "                res4 = res4 + res3\n",
    "                res4 = leaky_relu(res4)\n",
    "\n",
    "            with tf.variable_scope('conv3'):\n",
    "                conv3 = tf.layers.conv2d(res4, 128, [4, 4], [2 ,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                conv3 = leaky_relu(conv3)\n",
    "\n",
    "            num_res_itr = 3\n",
    "            layers = []\n",
    "            layers.append(conv3)\n",
    "            \n",
    "            depth = [128, 256, 512, 1024]\n",
    "            for i in range(int(num_res_itr)):\n",
    "                with tf.variable_scope('res_%d_1' % (i+1+4)):\n",
    "                    res = tf.layers.conv2d(layers[-1], depth[i], [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                    res = leaky_relu(res)\n",
    "                    res = tf.layers.conv2d(res, depth[i], [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                    res = leaky_relu(res)\n",
    "                    res = layers[-1] + res\n",
    "                    res = leaky_relu(res)\n",
    "                layers.append(res)\n",
    "\n",
    "                with tf.variable_scope('res_%d_2' % (i+1+4)):\n",
    "                    res = tf.layers.conv2d(layers[-1], depth[i], [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                    res = leaky_relu(res)\n",
    "                    res = tf.layers.conv2d(res, depth[i], [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                    res = leaky_relu(res)\n",
    "                    res = layers[-1] + res\n",
    "                    res = leaky_relu(res)\n",
    "\n",
    "                conv = tf.layers.conv2d(res, depth[i+1], [4, 4], [2 ,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                conv = leaky_relu(conv) \n",
    "                layers.append(conv)\n",
    "\n",
    "                \n",
    "            output = tf.layers.dense(layers[-1], 1, kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02)) \n",
    "            #output = tf.nn.sigmoid(output)\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='d')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_mb, Y_mbを収集\n",
    "def extractXimg(path, batch_size, img_size):\n",
    "    imgs = os.listdir(path)\n",
    "    rand_id = np.random.randint(0, len(imgs), size=batch_size)\n",
    "    X_mb = np.zeros((1, img_size, img_size, 3))\n",
    "    for i in range(batch_size):\n",
    "        img = Image.open(path+\"/\"+imgs[rand_id[i]])\n",
    "        background = Image.new(\"RGB\", img.size, (255,255,255))\n",
    "        background.paste(img, mask=img.split()[3])\n",
    "        img_np = np.asarray(background)\n",
    "        X_mb = np.vstack((X_mb, img_np[np.newaxis, :]))\n",
    "        img.close()\n",
    "        background.close()\n",
    "    X_mb = X_mb[1:,:]\n",
    "    return X_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DRAGAN:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 16\n",
    "        self.g = Generator(self.batch_size)\n",
    "        self.d = Discriminator()\n",
    "        self.z_dim = 128\n",
    "\n",
    "        self.img_size = 128\n",
    "        \n",
    "        self.epochs = 500000\n",
    "        self.epoch_saveMetrics = 100\n",
    "        self.epoch_saveSampleImg = 100\n",
    "        self.epoch_saveParameter = 10000\n",
    "        self.losses = {\"d_loss\":[], \"g_loss\":[]}\n",
    "\n",
    "        self.Z = tf.placeholder(tf.float32, [None, self.z_dim])\n",
    "        self.X_tr = tf.placeholder(tf.float32, [None, self.img_size*self.img_size*3])\n",
    "        self.X_p = tf.placeholder(tf.float32, [None, self.img_size*self.img_size*3])\n",
    "\n",
    "        self.X_g = self.g(self.Z, training=True)\n",
    "        self.Y_tr = self.d(self.X_tr, training=True)\n",
    "        self.Y_g = self.d(self.X_g, training=True)\n",
    "\n",
    "        self.noise = np.random.uniform(-1, 1, size=[self.batch_size, self.z_dim]).astype(np.float32)\n",
    "        self.L_g, self.L_d = self.loss()\n",
    "        self.learning_rate = tf.placeholder(\"float\", [])\n",
    "\n",
    "    def d_loss_calc(self, gradient_penalty):\n",
    "        lambda_ = 10\n",
    "\n",
    "        d_loss_train = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.Y_tr, labels=tf.ones_like(self.Y_tr)))\n",
    "        d_loss_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.Y_g, labels=tf.zeros_like(self.Y_g)))\n",
    "        loss = d_loss_train + d_loss_gen\n",
    "        loss = loss + lambda_*gradient_penalty\n",
    "        return loss\n",
    "\n",
    "    def g_loss_calc(self):\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.Y_g, labels=tf.ones_like(self.Y_g)))\n",
    "        return loss\n",
    "\n",
    "    def loss(self):\n",
    "        #z = tf.random_uniform([self.batch_size, self.z_dim], minval=-1.0, maxval=1.0)\n",
    "\n",
    "        alpha = tf.random_uniform(shape=[self.batch_size,1], minval=0., maxval=1.)\n",
    "        #print(alpha.get_shape())\n",
    "        #print(self.X_p.get_shape())\n",
    "        diff = self.X_p - self.X_tr\n",
    "        interpolates = self.X_tr + (alpha*diff)\n",
    "        gradients = tf.gradients(self.d(interpolates), [interpolates])[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "        gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "\n",
    "        return self.g_loss_calc(), self.d_loss_calc(gradient_penalty)\n",
    "\n",
    "    def train(self):\n",
    "        # Optimizer\n",
    "\n",
    "        #learning_rate = 1e-4\n",
    "        beta1 = 0.5\n",
    "        beta2 = 0.9\n",
    "       \n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=beta1, beta2=beta2)\n",
    "        d_train_op = d_opt.minimize(self.L_d, var_list=self.d.variables)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=beta1, beta2=beta2)\n",
    "        g_train_op = g_opt.minimize(self.L_g, var_list=self.g.variables)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        #%debug\n",
    "\n",
    "        config = tf.ConfigProto(\n",
    "            gpu_options=tf.GPUOptions(\n",
    "                visible_device_list=\"0\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            lr = 1e-4\n",
    "            for epoch in range(self.epochs):\n",
    "                if epoch > 2000:\n",
    "                    lr = 5e-5\n",
    "                if epoch > 4000:\n",
    "                    lr = 1e-5\n",
    "\n",
    "                if epoch % self.epoch_saveSampleImg == 0:\n",
    "                    imgs_gen = self.sample_images(inputs=self.noise).eval() \n",
    "                    #imgs_gen += 0.5\n",
    "                    imgs_gen *= 1\n",
    "                    print(\"saving images\")\n",
    "                    path = \"generated_figures_gpu0\"\n",
    "                    save_imgs(path, imgs_gen, epoch=epoch)\n",
    "\n",
    "                for _ in range(10):\n",
    "\n",
    "                    # 訓練データを抜粋\n",
    "                    Z_mb = np.random.uniform(-1, 1, size=[self.batch_size, self.z_dim]).astype(np.float32)\n",
    "\n",
    "                    X_mb = extractXimg(\"./128x128x3resized_faceimgs\", self.batch_size, self.img_size)\n",
    "                    X_mb = X_mb.reshape([self.batch_size, -1])\n",
    "                    X_mb = X_mb/255\n",
    "                    X_p = X_mb + 0.5*np.std(X_mb)*np.random.random(X_mb.shape)\n",
    "\n",
    "                    _, d_loss_value = sess.run([d_train_op, self.L_d], feed_dict={self.Z:Z_mb , self.X_tr: X_mb, self.X_p:X_p, self.learning_rate:lr})\n",
    "                _, g_loss_value, = sess.run([g_train_op, self.L_g], feed_dict={self.Z:Z_mb, self.X_tr: X_mb, self.X_p:X_p, self.learning_rate:lr})\n",
    "\n",
    "                # 結果をappend\n",
    "                self.losses[\"d_loss\"].append(d_loss_value)\n",
    "                self.losses[\"g_loss\"].append(g_loss_value)\n",
    "\n",
    "                # グラフの描画（余裕があったら）\n",
    "                if epoch % self.epoch_saveMetrics == 0:\n",
    "                    print(\"epoch:\" + str(epoch) + \", d_loss:\" + str(d_loss_value) + \", g_loss:\" + str(g_loss_value))\n",
    "                    path = \"metrics_gpu0\"\n",
    "                    save_metrics(path, self.losses, epoch)\n",
    "\n",
    "                if epoch % self.epoch_saveParameter == 0:\n",
    "                    path = \"model_gpu0\"\n",
    "                    if not os.path.isdir(path):\n",
    "                        os.makedirs(path)\n",
    "                    saver.save(sess, path + \"/dcgan_model\" + str(epoch) + \".ckpt\")\n",
    "\n",
    "    def sample_images(self, row=5, col=12, inputs=None, epoch=None):\n",
    "        images = self.g(inputs, training=True)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujitoko/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:72: DeprecationWarning: PyUnicode_AsEncodedObject() is deprecated; use PyUnicode_AsEncodedString() to encode from str to bytes or PyCodec_Encode() for generic encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving images\n",
      "epoch:0, d_loss:1.54122, g_loss:0.784011\n",
      "saving images\n",
      "epoch:100, d_loss:0.498339, g_loss:1.50246\n"
     ]
    }
   ],
   "source": [
    "gan = DRAGAN()\n",
    "gan.train()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
