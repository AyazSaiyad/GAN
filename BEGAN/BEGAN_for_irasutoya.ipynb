{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from model import *\n",
    "from utility import *\n",
    "\n",
    "model_name = \"BEGAN_for_irasutoya\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.g_bn0 = BatchNormalization(name = 'g_bn0')\n",
    "        self.g_bn1 = BatchNormalization(name = 'g_bn1')\n",
    "        self.g_bn2 = BatchNormalization(name = 'g_bn2')\n",
    "        self.g_bn3 = BatchNormalization(name = 'g_bn3')\n",
    "\n",
    "    def __call__(self, z):\n",
    "        with tf.variable_scope('g', reuse=self.reuse):\n",
    "            fc0 = full_connection_layer(z, 1024, name=\"fc0\")\n",
    "            fc0 = self.g_bn0(fc0)\n",
    "            fc0 = tf.nn.relu(fc0)\n",
    "\n",
    "            fc1 = full_connection_layer(fc0, 6*6*512, name=\"fc1\")\n",
    "            fc1 = self.g_bn1(fc1)\n",
    "            fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "            fc1 = tf.reshape(fc1, [-1, 6, 6, 512])\n",
    "\n",
    "            batch_size = tf.shape(fc1)[0]\n",
    "            deconv0 = deconv2d_layer(fc1, [batch_size, 12, 12, 256], kernel_size=5, name=\"deconv0\")\n",
    "\n",
    "            deconv0 = lrelu(deconv0, leak=0.3)\n",
    "\n",
    "            deconv1 = deconv2d_layer(deconv0, [batch_size, 24, 24, 128], kernel_size=5, name=\"deconv1\")\n",
    "            deconv1 = self.g_bn2(deconv1)\n",
    "            deconv1 = lrelu(deconv1, leak=0.3)\n",
    "\n",
    "            deconv2 = deconv2d_layer(deconv1, [batch_size, 48, 48, 64], kernel_size=5, name=\"deconv2\")\n",
    "            deconv2 = self.g_bn3(deconv2)\n",
    "            deconv2 = lrelu(deconv2, leak=0.3)\n",
    "\n",
    "            deconv3 = deconv2d_layer(deconv2, [batch_size, 96, 96, 3], kernel_size=5, name=\"deconv3\")\n",
    "\n",
    "            output = tf.nn.tanh(deconv3)\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='g')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.d_bn0 = BatchNormalization(name=\"d_bn0\")\n",
    "        self.d_bn1 = BatchNormalization(name=\"d_bn1\")\n",
    "        self.d_bn2 = BatchNormalization(name=\"d_bn2\")\n",
    "        self.d_bn3 = BatchNormalization(name=\"d_bn3\")\n",
    "        self.d_bn4 = BatchNormalization(name=\"d_bn4\")\n",
    "        self.d_bn5 = BatchNormalization(name=\"d_bn5\")\n",
    "        self.d_bn6 = BatchNormalization(name=\"d_bn6\")\n",
    "        self.d_bn7 = BatchNormalization(name=\"d_bn7\")\n",
    "        self.d_bn8 = BatchNormalization(name=\"d_bn8\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        with tf.variable_scope('d', reuse=self.reuse):\n",
    "            x = tf.reshape(x, [-1, 96, 96, 3])\n",
    "\n",
    "            conv1 = conv2d_layer(x, 64, kernel_size=5, name=\"d_conv0\")\n",
    "            conv1 = self.d_bn0(conv1)\n",
    "            conv1 = tf.nn.relu(conv1) # 48x48x64\n",
    "            \n",
    "            conv2 = conv2d_layer(conv1, 128, kernel_size=5, name=\"d_conv1\")\n",
    "            conv2 = self.d_bn1(conv2)\n",
    "            conv2 = tf.nn.relu(conv2) # 24x24x128\n",
    "\n",
    "            conv3 = conv2d_layer(conv2, 256, kernel_size=5, name=\"d_conv2\")\n",
    "            conv3 = self.d_bn2(conv3)\n",
    "            conv3 = tf.nn.relu(conv3) # 12x12x256\n",
    "\n",
    "            conv4 = conv2d_layer(conv3, 512, kernel_size=5, name=\"d_conv3\")\n",
    "            conv4 = self.d_bn3(conv4)\n",
    "            conv4 = tf.nn.relu(conv4) # 6x6x512\n",
    "            conv4 = tf.reshape(conv4, [-1, 6*6*512])\n",
    "\n",
    "            fc0 = full_connection_layer(conv4, 64, name=\"fc0\")\n",
    "            fc0 = self.d_bn4(fc0)\n",
    "            fc0 = tf.nn.relu(fc0)\n",
    "            \n",
    "            fc1 = full_connection_layer(fc0, 6*6*512, name=\"fc1\")\n",
    "            fc1 = self.d_bn5(fc1)\n",
    "            fc1 = tf.nn.relu(fc1)\n",
    "            fc1 = tf.reshape(fc1, [-1, 6, 6, 512])\n",
    "\n",
    "            batch_size = tf.shape(fc1)[0]\n",
    "            deconv0 = deconv2d_layer(fc1, [batch_size, 12, 12, 256], kernel_size=5, name=\"deconv0\")\n",
    "            deconv0 = self.d_bn6(deconv0)\n",
    "            deconv0 = tf.nn.relu(deconv0)    \n",
    "\n",
    "            deconv1 = deconv2d_layer(deconv0, [batch_size, 24, 24, 128], kernel_size=5, name=\"deconv1\")\n",
    "            deconv1 = self.d_bn7(deconv1)\n",
    "            deconv1 = tf.nn.relu(deconv1)\n",
    "\n",
    "            deconv2 = deconv2d_layer(deconv1, [batch_size, 48, 48, 64], kernel_size=5, name=\"deconv2\")\n",
    "            deconv2 = self.d_bn8(deconv2)\n",
    "            deconv2 = tf.nn.relu(deconv2)\n",
    "\n",
    "            deconv3 = deconv2d_layer(deconv2, [batch_size, 96, 96, 3], kernel_size=5, name=\"deconv3\")\n",
    "            output = tf.nn.tanh(deconv3)\n",
    "            \n",
    "            #batch_size = tf.shape(output)[0]\n",
    "            #recon_error = tf.sqrt(2*tf.nn.l2_loss(output - x))/batch_size\n",
    "            recon_error = tf.reduce_mean(tf.abs(output - x))\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='d')\n",
    "\n",
    "        return output, recon_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.img_size = 96\n",
    "        self.z_size = 100\n",
    "        \n",
    "        self.epochs = 50000\n",
    "        self.epoch_saveMetrics = 1000\n",
    "        self.epoch_saveSampleImg = 1000\n",
    "        self.epoch_saveParamter = 5000\n",
    "        self.losses = {\"d_loss\":[], \"g_loss\":[], \"M_value\":[]}\n",
    "\n",
    "        # unrolled counts\n",
    "        self.steps = 5\n",
    "\n",
    "        self.dataset = np.load(\"irasutoya_face_1813x96x96x3_jpg.npy\")\n",
    "        self.dataset = (self.dataset/255) - 0.5\n",
    "\n",
    "        self.X_tr = tf.placeholder(tf.float32, shape=[None, self.img_size, self.img_size, 3])\n",
    "        self.z = tf.placeholder(tf.float32, [None, self.z_size])\n",
    "        \n",
    "        self.g = Generator()\n",
    "        self.d = Discriminator()\n",
    "        self.Xg = self.g(self.z)\n",
    "\n",
    "        self.k = tf.Variable(0., trainable=False)\n",
    "        self.lambda_ = 1e-3\n",
    "        self.gamma = 0.75\n",
    "        \n",
    "    def loss(self):\n",
    "        output_tr, recon_error_tr = self.d(self.X_tr)\n",
    "        output_gen, recon_error_gen = self.d(self.Xg)\n",
    "        \n",
    "        loss_d = recon_error_tr - self.k*recon_error_gen\n",
    "        loss_g = recon_error_gen\n",
    "\n",
    "        self.M = recon_error_tr + tf.abs(self.gamma*recon_error_tr - recon_error_gen)\n",
    "        self.update_k = self.k.assign(self.k + self.lambda_*(self.gamma*recon_error_tr - recon_error_gen))\n",
    "\n",
    "        return loss_g, loss_d\n",
    "\n",
    "    def train(self):\n",
    "        # Optimizer\n",
    "        d_lr = 2e-4\n",
    "        d_beta1 = 0.5\n",
    "        g_lr = 2e-4\n",
    "        g_beta1 = 0.5\n",
    "\n",
    "        self.L_g, self.L_d = self.loss()\n",
    "\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate=d_lr)\n",
    "        d_train_op = d_opt.minimize(self.L_d, var_list=self.d.variables)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=g_lr)\n",
    "        g_train_op = g_opt.minimize(self.L_g, var_list=self.g.variables)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        config = tf.ConfigProto(\n",
    "            gpu_options=tf.GPUOptions(\n",
    "                visible_device_list= \"0\"\n",
    "            )\n",
    "        )\n",
    "                \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # preparing noise vec for test\n",
    "            bs = 100\n",
    "            test_z = np.random.uniform(-1, 1, size=[bs, self.z_size])\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "\n",
    "                # extract images for training\n",
    "                rand_index = np.random.randint(0, self.dataset.shape[0], size=self.batch_size)\n",
    "                X_mb = self.dataset[rand_index, :].astype(np.float32)\n",
    "                X_mb = np.reshape(X_mb, [-1, 96, 96, 3])\n",
    "\n",
    "                z = np.random.uniform(-1, 1, size=[self.batch_size, self.z_size])\n",
    "\n",
    "                # train Discriminator\n",
    "                _, d_loss_value = sess.run([d_train_op, self.L_d], feed_dict={\n",
    "                    self.X_tr: X_mb,\n",
    "                    self.z:z,\n",
    "                })\n",
    "\n",
    "                # train Generator\n",
    "                _, g_loss_value = sess.run([g_train_op, self.L_g], feed_dict={\n",
    "                    self.X_tr: X_mb,\n",
    "                    self.z:z,\n",
    "                })\n",
    "\n",
    "                # update k\n",
    "                M_value, _ = sess.run([self.M, self.update_k], feed_dict={\n",
    "                    self.X_tr: X_mb,\n",
    "                    self.z:z,\n",
    "                })\n",
    "\n",
    "                # append loss value for visualizing\n",
    "                self.losses[\"d_loss\"].append(np.sum(d_loss_value))\n",
    "                self.losses[\"g_loss\"].append(np.sum(g_loss_value))\n",
    "                self.losses[\"M_value\"].append(M_value)\n",
    "                \n",
    "                # print epoch\n",
    "                if epoch % 100 == 0:\n",
    "                    print('epoch:{0}, d_loss:{1}, g_loss:{2}, M:value:{3} '.format(epoch, d_loss_value, g_loss_value, M_value))\n",
    "                \n",
    "                # visualize loss\n",
    "                if epoch % self.epoch_saveMetrics == 0:\n",
    "                    save_metrics(model_name, self.losses, epoch)\n",
    "\n",
    "                # visualize generated images during training\n",
    "                if epoch % self.epoch_saveSampleImg == 0:\n",
    "                    img = sess.run(self.Xg, feed_dict={self.z: test_z})\n",
    "                    img = (img*0.5) + 0.5\n",
    "                    save_imgs(model_name, img, name=str(epoch))\n",
    "\n",
    "                # save model parameters \n",
    "                if epoch % self.epoch_saveParamter == 0:\n",
    "                    dir_path = \"model_\" + model_name\n",
    "                    if not os.path.isdir(dir_path):\n",
    "                        os.makedirs(dir_path)\n",
    "\n",
    "                    saver.save(sess, dir_path + \"/\" + str(epoch) + \".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujitoko/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:60: DeprecationWarning: PyUnicode_AsEncodedObject() is deprecated; use PyUnicode_AsEncodedString() to encode from str to bytes or PyCodec_Encode() for generic encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, d_loss:0.42656436562538147, g_loss:0.2646520435810089, M:value:0.460518479347229 \n",
      "epoch:100, d_loss:0.08670857548713684, g_loss:0.07593969255685806, M:value:0.09418926388025284 \n",
      "epoch:200, d_loss:0.07160013169050217, g_loss:0.06475510448217392, M:value:0.07781708985567093 \n",
      "epoch:300, d_loss:0.06462905555963516, g_loss:0.0506284236907959, M:value:0.06408795714378357 \n",
      "epoch:400, d_loss:0.06083686277270317, g_loss:0.046965714544057846, M:value:0.06062028184533119 \n",
      "epoch:500, d_loss:0.057797156274318695, g_loss:0.03979562222957611, M:value:0.06076148524880409 \n",
      "epoch:600, d_loss:0.05294986441731453, g_loss:0.03910772129893303, M:value:0.05293484777212143 \n",
      "epoch:700, d_loss:0.04869881644845009, g_loss:0.03582185506820679, M:value:0.049931496381759644 \n",
      "epoch:800, d_loss:0.05000486224889755, g_loss:0.0343509204685688, M:value:0.05283728241920471 \n",
      "epoch:900, d_loss:0.04677318409085274, g_loss:0.034027110785245895, M:value:0.047986630350351334 \n",
      "epoch:1000, d_loss:0.04616539552807808, g_loss:0.033948928117752075, M:value:0.04798576235771179 \n",
      "epoch:1100, d_loss:0.04290104657411575, g_loss:0.03140614926815033, M:value:0.04325975477695465 \n",
      "epoch:1200, d_loss:0.04386322200298309, g_loss:0.035256363451480865, M:value:0.044582974165678024 \n",
      "epoch:1300, d_loss:0.04181915894150734, g_loss:0.03004591539502144, M:value:0.04319579899311066 \n",
      "epoch:1400, d_loss:0.038746654987335205, g_loss:0.03234283998608589, M:value:0.04047921299934387 \n",
      "epoch:1500, d_loss:0.03892473503947258, g_loss:0.030656972900032997, M:value:0.03856189548969269 \n",
      "epoch:1600, d_loss:0.04004346579313278, g_loss:0.031215142458677292, M:value:0.03971298784017563 \n",
      "epoch:1700, d_loss:0.03963224217295647, g_loss:0.030665980651974678, M:value:0.03973376750946045 \n",
      "epoch:1800, d_loss:0.03763164207339287, g_loss:0.03142404556274414, M:value:0.03907604515552521 \n",
      "epoch:1900, d_loss:0.03819135203957558, g_loss:0.0377034991979599, M:value:0.04271361604332924 \n",
      "epoch:2000, d_loss:0.03647877275943756, g_loss:0.027264026924967766, M:value:0.03571568801999092 \n",
      "epoch:2100, d_loss:0.04104052111506462, g_loss:0.0296120997518301, M:value:0.039293382316827774 \n"
     ]
    }
   ],
   "source": [
    "gan = GAN()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
