{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from model import *\n",
    "from utility import *\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "model_name = \"UnrolledGAN_for_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.g_bn0 = BatchNormalization(name = 'g_bn0')\n",
    "\n",
    "    def __call__(self, z, training=False):\n",
    "        with tf.variable_scope('g', reuse=self.reuse):\n",
    "            fc0 = full_connection_layer(z, 7*7*512, name=\"fc0\")\n",
    "            fc0 = tf.reshape(fc0, [-1, 7, 7, 512])\n",
    "\n",
    "            batch_size = tf.shape(fc0)[0]\n",
    "            deconv0 = deconv2d_layer(fc0, [batch_size, 14, 14, 256], kernel_size=4, name=\"deconv0\")\n",
    "            deconv0 = self.g_bn0(deconv0)\n",
    "            deconv0 = lrelu(deconv0, leak=0.3)\n",
    "\n",
    "            deconv1 = deconv2d_layer(deconv0, [batch_size, 28, 28, 1], kernel_size=4, name=\"deconv1\")\n",
    "            deconv1 = tf.nn.tanh(deconv1)\n",
    "            output = deconv1 \n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='g')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.d_bn0 = BatchNormalization(name=\"d_bn0\")\n",
    "        self.d_bn1 = BatchNormalization(name=\"d_bn1\")\n",
    "        \n",
    "    def __call__(self, x,training=False, name=''):\n",
    "        with tf.variable_scope('d', reuse=self.reuse):\n",
    "            x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "            conv1 = conv2d_layer(x, 128, kernel_size=4, name=\"d_conv0\")\n",
    "            conv1 = self.d_bn0(conv1)\n",
    "            conv1 = lrelu(conv1, leak=0.3)\n",
    "            \n",
    "            conv2 = conv2d_layer(conv1, 256, kernel_size=4, name=\"d_conv1\")\n",
    "            conv2 = self.d_bn1(conv2)\n",
    "            conv2 = lrelu(conv2, leak=0.3)\n",
    "            conv2 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "            fc0 = full_connection_layer(conv2, 512, name=\"fc0\")\n",
    "            fc0 = lrelu(fc0)\n",
    "\n",
    "            fc1 = full_connection_layer(fc0, 128, name=\"fc1\")\n",
    "            fc1 = lrelu(fc1)\n",
    "\n",
    "            disc = full_connection_layer(fc1, 1, name = 'disc')\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='d')\n",
    "\n",
    "        return disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.img_size = 28\n",
    "        self.z_size = 50\n",
    "        \n",
    "        self.epochs = 50000\n",
    "        self.epoch_saveMetrics = 1000\n",
    "        self.epoch_saveSampleImg = 1000\n",
    "        self.epoch_saveParamter = 5000\n",
    "        self.losses = {\"d_loss\":[], \"g_loss\":[]}\n",
    "\n",
    "        # unrolled counts\n",
    "        self.steps = 5\n",
    "\n",
    "        self.X_tr = tf.placeholder(tf.float32, shape=[None, self.img_size, self.img_size, 1])\n",
    "        self.z = tf.placeholder(tf.float32, [None, self.z_size])\n",
    "        \n",
    "        self.g = Generator()\n",
    "        self.d = Discriminator()\n",
    "        self.Xg = self.g(self.z)\n",
    "\n",
    "    def loss(self):\n",
    "        disc_tr = self.d(self.X_tr)\n",
    "        disc_gen = self.d(self.Xg)\n",
    "        \n",
    "        loss_d_tr = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_tr, labels=tf.ones_like(disc_tr)))\n",
    "        loss_d_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen, labels=tf.zeros_like(disc_gen)))\n",
    "        loss_d = (loss_d_tr + loss_d_gen)/2\n",
    "        \n",
    "        loss_g = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen, labels=tf.ones_like(disc_gen)))\n",
    "        \n",
    "        return loss_g, loss_d\n",
    "\n",
    "    def train(self):\n",
    "        # Optimizer\n",
    "        d_lr = 2e-4\n",
    "        d_beta1 = 0.1\n",
    "        g_lr = 1e-4\n",
    "        g_beta1 = 0.5\n",
    "\n",
    "        self.L_g, self.L_d = self.loss()\n",
    "\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate=d_lr)\n",
    "        d_train_op = d_opt.minimize(self.L_d, var_list=self.d.variables)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=g_lr)\n",
    "        g_train_op = g_opt.minimize(self.L_g, var_list=self.g.variables)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        config = tf.ConfigProto(\n",
    "            gpu_options=tf.GPUOptions(\n",
    "                visible_device_list= \"0\"\n",
    "            )\n",
    "        )\n",
    "                \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # preparing noise vec for test\n",
    "            bs = 100\n",
    "            test_z = np.random.uniform(-1, 1, size=[bs, self.z_size])\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                for step in range(self.steps):\n",
    "                    # extract images for training\n",
    "                    X_mb, _ = mnist.train.next_batch(self.batch_size)\n",
    "                    X_mb = np.reshape(X_mb, [-1, 28, 28, 1])\n",
    "\n",
    "                    z = np.random.uniform(-1, 1, size=[self.batch_size, self.z_size])\n",
    "\n",
    "                    # train Discriminator\n",
    "                    _, d_loss_value = sess.run([d_train_op, self.L_d], feed_dict={\n",
    "                        self.X_tr: X_mb,\n",
    "                        self.z:z,\n",
    "                    })\n",
    "\n",
    "                    # train Generator\n",
    "                    _, g_loss_value = sess.run([g_train_op, self.L_g], feed_dict={\n",
    "                        self.X_tr: X_mb,\n",
    "                        self.z:z,\n",
    "                    })\n",
    "\n",
    "                    # save parameters of discriminator network\n",
    "                    if step == 0:\n",
    "                        dis_var = sess.run(self.d.variables)\n",
    "\n",
    "                # reload parametes of discriminator network\n",
    "                dis_var_np = np.array(dis_var)\n",
    "                for (dst, src) in zip(self.d.variables, dis_var_np):\n",
    "                    dst = src\n",
    "\n",
    "                #sess.run(self.d.variables)\n",
    "                del dis_var_np\n",
    "\n",
    "                # append loss value for visualizing\n",
    "                self.losses[\"d_loss\"].append(np.sum(d_loss_value))\n",
    "                self.losses[\"g_loss\"].append(np.sum(g_loss_value))\n",
    "                \n",
    "                # print epoch\n",
    "                if epoch % 50 == 0:\n",
    "                    print('epoch:{0}, d_loss:{1}, g_loss{2} '.format(epoch, d_loss_value, g_loss_value))\n",
    "                \n",
    "                # visualize loss\n",
    "                if epoch % self.epoch_saveMetrics == 0:\n",
    "                    save_metrics(model_name, self.losses, epoch)\n",
    "\n",
    "                # visualize generated images during training\n",
    "                if epoch % self.epoch_saveSampleImg == 0:\n",
    "                    img = sess.run(self.Xg, feed_dict={self.z: test_z})\n",
    "                    save_imgs(model_name, img, name=str(epoch))\n",
    "\n",
    "                # save model parameters \n",
    "                if epoch % self.epoch_saveParamter == 0:\n",
    "                    dir_path = \"model_\" + model_name\n",
    "                    if not os.path.isdir(dir_path):\n",
    "                        os.makedirs(dir_path)\n",
    "\n",
    "                    saver.save(sess, dir_path + \"/\" + str(epoch) + \".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujitoko/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:53: DeprecationWarning: PyUnicode_AsEncodedObject() is deprecated; use PyUnicode_AsEncodedString() to encode from str to bytes or PyCodec_Encode() for generic encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, d_loss:0.3525939881801605, g_loss1.286449670791626 \n",
      "epoch:50, d_loss:0.04732871055603027, g_loss6.387814521789551 \n",
      "epoch:100, d_loss:0.07679566740989685, g_loss7.958855628967285 \n",
      "epoch:150, d_loss:0.09552913159132004, g_loss10.210464477539062 \n",
      "epoch:200, d_loss:0.04023100435733795, g_loss8.502360343933105 \n",
      "epoch:250, d_loss:0.012644374743103981, g_loss7.25709342956543 \n",
      "epoch:300, d_loss:0.009340484626591206, g_loss7.5868449211120605 \n",
      "epoch:350, d_loss:0.000994242960587144, g_loss9.228678703308105 \n",
      "epoch:400, d_loss:0.0002080320700770244, g_loss11.19413948059082 \n",
      "epoch:450, d_loss:4.862185596721247e-05, g_loss15.198783874511719 \n",
      "epoch:500, d_loss:0.000680685683619231, g_loss10.065227508544922 \n",
      "epoch:550, d_loss:0.001341699156910181, g_loss8.564330101013184 \n",
      "epoch:600, d_loss:0.0003470972296781838, g_loss10.796158790588379 \n",
      "epoch:650, d_loss:0.004341694060713053, g_loss8.828575134277344 \n",
      "epoch:700, d_loss:0.0005484457360580564, g_loss9.616558074951172 \n",
      "epoch:750, d_loss:0.0041166022419929504, g_loss8.167240142822266 \n",
      "epoch:800, d_loss:0.0017002248205244541, g_loss12.942678451538086 \n",
      "epoch:850, d_loss:0.0002896477817557752, g_loss13.520437240600586 \n",
      "epoch:900, d_loss:0.0005032657063566148, g_loss12.196985244750977 \n",
      "epoch:950, d_loss:0.00017008719441946596, g_loss9.185416221618652 \n",
      "epoch:1000, d_loss:0.00144023762550205, g_loss9.551414489746094 \n",
      "epoch:1050, d_loss:0.002778352703899145, g_loss8.974231719970703 \n",
      "epoch:1100, d_loss:0.00011520558473421261, g_loss11.36742115020752 \n",
      "epoch:1150, d_loss:0.15534408390522003, g_loss9.942764282226562 \n",
      "epoch:1200, d_loss:0.007621070370078087, g_loss7.326040267944336 \n",
      "epoch:1250, d_loss:0.006560232490301132, g_loss7.601049423217773 \n",
      "epoch:1300, d_loss:0.0006396917160600424, g_loss11.016780853271484 \n",
      "epoch:1350, d_loss:0.008433443494141102, g_loss6.703188896179199 \n",
      "epoch:1400, d_loss:0.017412591725587845, g_loss6.436180114746094 \n"
     ]
    }
   ],
   "source": [
    "gan = GAN()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
