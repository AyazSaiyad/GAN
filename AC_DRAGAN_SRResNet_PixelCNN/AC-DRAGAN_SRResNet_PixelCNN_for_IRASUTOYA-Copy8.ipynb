{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"AC-DRAGAN_SRResNet_PixelCNN_for_IRASUTOYA_gpu0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from model import *\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.g_bn0 = BatchNormalization(name = 'g_bn0')\n",
    "\n",
    "        self.num_res_blocks = 16\n",
    "        self.num_pixel_CNN_blocks = 3\n",
    "        \n",
    "        self.res_bns = []\n",
    "        for i in range(int(self.num_res_blocks)):\n",
    "            self.res_bns.append(BatchNormalization(name = \"res_%d\" % (2*i)))\n",
    "            self.res_bns.append(BatchNormalization(name = \"res_%d\" % (2*i+1)))\n",
    "        \n",
    "        self.ps_bns = []\n",
    "        for i in range(int(self.num_pixel_CNN_blocks)):\n",
    "            self.ps_bns.append(BatchNormalization(name = \"ps_%d\" % i))\n",
    "        \n",
    "        self.g_bn1 = BatchNormalization(name = 'g_bn1')\n",
    "        \n",
    "    def __call__(self, z):\n",
    "        with tf.variable_scope('g', reuse=self.reuse):\n",
    "            \n",
    "            # reshape from inputs\n",
    "            with tf.variable_scope('fc0'):\n",
    "                #z0 = tf.reshape(z, [-1, self.z_dim])\n",
    "                fc0 = full_connection_layer(z, 64*16*16, name=\"fc0\")\n",
    "                fc0 = self.g_bn0(fc0)\n",
    "                fc0 = tf.nn.relu(fc0)\n",
    "                fc0 = tf.reshape(fc0, [-1,16,16,64])\n",
    "\n",
    "            assert fc0.get_shape().as_list()[1:] == [16,16,64]\n",
    "            \n",
    "            layers = []\n",
    "            layers.append(fc0)\n",
    "            \n",
    "            for i in range(int(self.num_res_blocks)):\n",
    "                with tf.variable_scope('res_%d' % (i+1)):\n",
    "                    res = conv2d_layer(layers[-1], 64, kernel_size=3, strides=1, name=\"g_conv_res_%d\" % (2*i))\n",
    "                    res = self.res_bns[2*i](res)\n",
    "                    res = tf.nn.relu(res)\n",
    "\n",
    "                    res = conv2d_layer(res, 64, kernel_size=3, strides=1, name=\"g_conv_res_%d\" % (2*i+1))\n",
    "                    res = self.res_bns[2*i+1](res)\n",
    "                    res = layers[-1] + res\n",
    "                    layers.append(res)                    \n",
    "\n",
    "            assert layers[-1].get_shape().as_list()[1:] == [16,16,64]\n",
    "            \n",
    "            with tf.variable_scope('conv17'):\n",
    "                conv17 = conv2d_layer(layers[-1], 64, kernel_size=3, strides=1, name=\"g_conv_17\")\n",
    "                conv17 = self.g_bn1(conv17)\n",
    "                conv17 = tf.nn.relu(conv17)\n",
    "                conv17 = layers[0] + conv17\n",
    "                layers.append(conv17)\n",
    "\n",
    "            assert layers[-1].get_shape().as_list()[1:] == [16, 16, 64]\n",
    "\n",
    "            for i in range(int(self.num_pixel_CNN_blocks)):\n",
    "                with tf.variable_scope('pixel_CNN_%d' % (i+1)):\n",
    "                    ps = conv2d_layer(layers[-1], 256, kernel_size=3, strides=1, name=\"g_conv_ps_%d\" % (i))\n",
    "                    ps = pixel_shuffle_layer(ps, 2, 64)\n",
    "                    ps = self.ps_bns[i](ps)\n",
    "                    ps = tf.nn.relu(ps)\n",
    "                    layers.append(ps)\n",
    "\n",
    "            assert layers[-1].get_shape().as_list()[1:] == [128, 128, 64]\n",
    "                    \n",
    "            with tf.variable_scope('output'):\n",
    "                output = conv2d_layer(layers[-1], 3, kernel_size=9, strides=1, name=\"output\")\n",
    "                output = tf.nn.sigmoid(output)\n",
    "\n",
    "            assert output.get_shape().as_list()[1:] == [128, 128, 3]            \n",
    "            \n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='g')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, cat_size):\n",
    "        self.reuse = False\n",
    "        self.d_bn0 = BatchNormalization(name=\"d_bn0\")\n",
    "        self.d_bn2 = BatchNormalization(name=\"d_bn2\")\n",
    "        self.d_bn3 = BatchNormalization(name=\"d_bn3\")\n",
    "        self.d_bn4 = BatchNormalization(name=\"d_bn4\")\n",
    "        \n",
    "        self.cat_size = cat_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        def leaky_relu(x):\n",
    "            return lrelu(x, leak=0.2)\n",
    "\n",
    "        with tf.variable_scope('d', reuse=self.reuse):\n",
    "           \n",
    "            x = tf.reshape(x, [-1, 128, 128, 3])\n",
    "            #y = tf.reshape(y, [-1, 1, 1, 4])\n",
    "            #y = tf.tile(y, [1, 64, 64, 1])\n",
    "            #print(y.shape)\n",
    "                \n",
    "            with tf.variable_scope('conv1'):\n",
    "                conv1 = tf.layers.conv2d(x, 32, [4, 4], [2 ,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                conv1 = leaky_relu(conv1)\n",
    "\n",
    "            #print(conv1.shape)\n",
    "            #conv1 = tf.concat([conv1, y], 3)\n",
    "            #conv1 = tf.layers.conv2d(conv1, 32, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "\n",
    "            with tf.variable_scope('res1'):\n",
    "                res1 = tf.layers.conv2d(conv1, 32, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res1 = leaky_relu(res1)\n",
    "                res1 = tf.layers.conv2d(res1, 32, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res1 = res1 + conv1\n",
    "                res1 = leaky_relu(res1)\n",
    "\n",
    "            with tf.variable_scope('res2'):\n",
    "                res2 = tf.layers.conv2d(res1, 32, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res2 = leaky_relu(res2)\n",
    "                res2 = tf.layers.conv2d(res2, 32, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res2 = res2 + res1\n",
    "                res2 = leaky_relu(res2)\n",
    "\n",
    "            with tf.variable_scope('conv2'):\n",
    "                conv2 = tf.layers.conv2d(res2, 64, [4, 4], [2 ,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                conv2 = leaky_relu(conv2)\n",
    "\n",
    "            with tf.variable_scope('res3'):\n",
    "                res3 = tf.layers.conv2d(conv2, 64, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res3 = leaky_relu(res3)\n",
    "                res3 = tf.layers.conv2d(res3, 64, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res3 = leaky_relu(res3)\n",
    "                res3 = res3 + conv2\n",
    "                res3 = leaky_relu(res3)\n",
    "\n",
    "            with tf.variable_scope('res4'):\n",
    "                res4 = tf.layers.conv2d(res3, 64, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res4 = leaky_relu(res4)\n",
    "                res4 = tf.layers.conv2d(res4, 64, [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                res4 = leaky_relu(res4)\n",
    "                res4 = res4 + res3\n",
    "                res4 = leaky_relu(res4)\n",
    "\n",
    "            with tf.variable_scope('conv3'):\n",
    "                conv3 = tf.layers.conv2d(res4, 128, [4, 4], [2 ,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                conv3 = leaky_relu(conv3)\n",
    "\n",
    "            num_res_itr = 3\n",
    "            layers = []\n",
    "            layers.append(conv3)\n",
    "            \n",
    "            depth = [128, 256, 512, 1024]\n",
    "            for i in range(int(num_res_itr)):\n",
    "                with tf.variable_scope('res_%d_1' % (i+1+4)):\n",
    "                    res = tf.layers.conv2d(layers[-1], depth[i], [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                    res = leaky_relu(res)\n",
    "                    res = tf.layers.conv2d(res, depth[i], [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                    res = leaky_relu(res)\n",
    "                    res = layers[-1] + res\n",
    "                    res = leaky_relu(res)\n",
    "                layers.append(res)\n",
    "\n",
    "                with tf.variable_scope('res_%d_2' % (i+1+4)):\n",
    "                    res = tf.layers.conv2d(layers[-1], depth[i], [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                    res = leaky_relu(res)\n",
    "                    res = tf.layers.conv2d(res, depth[i], [3,3], [1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                    res = leaky_relu(res)\n",
    "                    res = layers[-1] + res\n",
    "                    res = leaky_relu(res)\n",
    "\n",
    "                conv = tf.layers.conv2d(res, depth[i+1], [4, 4], [2 ,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "                conv = leaky_relu(conv) \n",
    "                layers.append(conv)\n",
    "\n",
    "            disc = full_connection_layer(layers[-1], 1, name=\"disc\")\n",
    "            aux = full_connection_layer(layers[-1], self.cat_size, name=\"aux\")\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='d')\n",
    "\n",
    "        return disc, aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.img_size = 128\n",
    "        self.rand_size = 100\n",
    "        self.cat_size = 4\n",
    "        self.z_size = self.rand_size + self.cat_size\n",
    "        \n",
    "        self.epochs = 50000\n",
    "        self.epoch_saveMetrics = 50\n",
    "        self.epoch_saveSampleImg = 50\n",
    "        self.epoch_saveParamter = 5000\n",
    "        self.losses = {\"d_loss\":[], \"g_loss\":[]}\n",
    "\n",
    "        # unrolled counts\n",
    "        self.steps = 5\n",
    "\n",
    "        #self.dataset = np.load(\"irasutoya_face_1813x96x96x3_jpg.npy\")\n",
    "        #self.dataset = (self.dataset/255)# - 0.5\n",
    "\n",
    "        self.X_tr = tf.placeholder(tf.float32, shape=[None, self.img_size*self.img_size*3])\n",
    "        self.Y_tr = tf.placeholder(tf.float32, shape=[None, self.cat_size])\n",
    "        self.z = tf.placeholder(tf.float32, [None, self.z_size])\n",
    "        self.X_per = tf.placeholder(tf.float32, shape=[None, self.img_size*self.img_size*3])\n",
    "\n",
    "        self.g = Generator()\n",
    "        self.d = Discriminator(self.cat_size)\n",
    "        self.Xg = self.g(self.z)\n",
    "        #self.dtd = DTD()\n",
    "        self.irasutoya = IRASUTOYA()\n",
    "\n",
    "    def loss(self):\n",
    "        disc_tr, aux_tr = self.d(self.X_tr)\n",
    "        disc_gen, aux_gen = self.d(self.Xg)\n",
    "        \n",
    "        lambda_adv = 1\n",
    "        lambda_gp = 10\n",
    "        lambda_c = 1\n",
    "       \n",
    "        loss_g = lambda_adv*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen, labels=tf.ones_like(disc_gen)))\n",
    "        #loss_g = lambda_adv*tf.reduce_mean(1 - tf.log(disc_gen + TINY))\n",
    "\n",
    "        diff = self.X_per - self.X_tr\n",
    "        #print(g_outputs.shape[0])\n",
    "        alpha = tf.random_uniform(shape=[self.batch_size,1], minval=0., maxval=1.)\n",
    "        interpolates = self.X_tr + (alpha*diff)\n",
    "        disc_interplates, _ = self.d(interpolates)\n",
    "        gradients = tf.gradients(disc_interplates, [interpolates])[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "        gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "\n",
    "        loss_d_tr = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_tr, labels=tf.ones_like(disc_tr)))\n",
    "        loss_d_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen, labels=tf.zeros_like(disc_gen)))\n",
    "        loss_d = lambda_adv*(loss_d_tr + loss_d_gen)\n",
    "        loss_d += lambda_gp*gradient_penalty\n",
    "\n",
    "        loss_c_tr = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=aux_tr, labels=self.Y_tr))\n",
    "        loss_c_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=aux_gen, labels=self.Y_tr))\n",
    "        loss_c = (loss_c_tr + loss_c_gen)\n",
    "\n",
    "        loss_g += loss_c*lambda_c\n",
    "        loss_d += loss_c*lambda_c\n",
    "        return loss_g, loss_d\n",
    "\n",
    "    def train(self):\n",
    "        # Optimizer\n",
    "        d_lr = 1e-4\n",
    "        d_beta1 = 0.5\n",
    "        g_lr = 1e-4\n",
    "        g_beta1 = 0.5\n",
    "\n",
    "        self.L_g, self.L_d = self.loss()\n",
    "\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate=d_lr)\n",
    "        d_train_op = d_opt.minimize(self.L_d, var_list=self.d.variables)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=g_lr)\n",
    "        g_train_op = g_opt.minimize(self.L_g, var_list=self.g.variables)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        config = tf.ConfigProto(\n",
    "            gpu_options=tf.GPUOptions(\n",
    "                visible_device_list= \"0\"\n",
    "            )\n",
    "        )\n",
    "                \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # preparing noise vec for test\n",
    "            bs = 100\n",
    "            test_z = np.random.uniform(0, 1, size=[bs, self.z_size])\n",
    "\n",
    "            # cat 0\n",
    "            bs = 10\n",
    "            test_cat0_rand = np.random.uniform(0, 1, size=[bs, self.rand_size])\n",
    "            test_cat0_cat = np.zeros([bs, self.cat_size]) + 0.5\n",
    "            test_cat0_cat[:, 0] = np.linspace(0, 1, num=bs)\n",
    "            test_cat0_z = np.concatenate((test_cat0_rand, test_cat0_cat), axis=1)  \n",
    "\n",
    "            # cat 1\n",
    "            bs = 10\n",
    "            #test_cat1_rand = np.random.uniform(-1, 1, size=[bs, self.rand_size])\n",
    "            test_cat1_cat = np.zeros([bs, self.cat_size]) + 0.5\n",
    "            test_cat1_cat[:, 1] = np.linspace(0, 1, num=bs)\n",
    "            test_cat1_z = np.concatenate((test_cat0_rand, test_cat1_cat), axis=1)  \n",
    "\n",
    "            # cat 2\n",
    "            bs = 10\n",
    "            #test_cat1_rand = np.random.uniform(-1, 1, size=[bs, self.rand_size])\n",
    "            test_cat2_cat = np.zeros([bs, self.cat_size]) + 0.5\n",
    "            test_cat2_cat[:, 2] = np.linspace(0, 1, num=bs)\n",
    "            test_cat2_z = np.concatenate((test_cat0_rand, test_cat2_cat), axis=1)\n",
    "\n",
    "            # cat 3\n",
    "            bs = 10\n",
    "            #test_cat1_rand = np.random.uniform(-1, 1, size=[bs, self.rand_size])\n",
    "            test_cat3_cat = np.zeros([bs, self.cat_size]) + 0.5\n",
    "            test_cat3_cat[:, 3] = np.linspace(0, 1, num=bs)\n",
    "            test_cat3_z = np.concatenate((test_cat0_rand, test_cat3_cat), axis=1)\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "\n",
    "                # visualize generated images during training\n",
    "                if epoch % self.epoch_saveSampleImg == 0:\n",
    "                    # rand\n",
    "                    img = sess.run(self.Xg, feed_dict={self.z: test_z})\n",
    "                    save_imgs(model_name, img, name=str(epoch)+\"_rand\")\n",
    "\n",
    "                    # cat\n",
    "                    test_cat = np.concatenate((test_cat0_z, test_cat1_z, test_cat2_z, test_cat3_z), axis=0)\n",
    "                    img = sess.run(self.Xg, feed_dict={self.z: test_cat})\n",
    "                    save_imgs(model_name, img, plot_dim=(4,10), size=(20, 8), name=str(epoch)+\"_cat\")\n",
    "\n",
    "                for step in range(self.steps):\n",
    "                    # extract images for training\n",
    "                    #rand_index = np.random.randint(0, self.dataset.shape[0], size=self.batch_size)\n",
    "                    #X_mb, Y_mb = self.dataset[rand_index, :].astype(np.float32)\n",
    "                    X_mb, Y_mb = self.irasutoya.extract(self.batch_size, self.img_size)\n",
    "                    X_mb = np.reshape(X_mb, [self.batch_size, -1])\n",
    "                    X_mb_per = X_mb + 0.5*np.std(X_mb)*np.random.random(X_mb.shape)\n",
    "\n",
    "                    rand = np.random.uniform(0, 1, size=[self.batch_size, self.rand_size])\n",
    "                    #print(rand.shape)\n",
    "                    #print(Y_mb.shape)\n",
    "                    z = np.hstack((rand, Y_mb))\n",
    "                    #print(z.shape)\n",
    "\n",
    "                    # train Discriminator\n",
    "                    _, d_loss_value = sess.run([d_train_op, self.L_d], feed_dict={\n",
    "                        self.X_tr: X_mb,\n",
    "                        self.z:z,\n",
    "                        self.Y_tr: Y_mb,\n",
    "                        self.X_per: X_mb_per,\n",
    "                    })\n",
    "\n",
    "                # train Generator\n",
    "                _, g_loss_value = sess.run([g_train_op, self.L_g], feed_dict={\n",
    "                    self.X_tr: X_mb,\n",
    "                    self.z:z,\n",
    "                    self.Y_tr: Y_mb,\n",
    "                    self.X_per: X_mb_per,\n",
    "                })\n",
    "\n",
    "                # append loss value for visualizing\n",
    "                self.losses[\"d_loss\"].append(np.sum(d_loss_value))\n",
    "                self.losses[\"g_loss\"].append(np.sum(g_loss_value))\n",
    "                \n",
    "                # print epoch\n",
    "                if epoch % 1 == 0:\n",
    "                    print('epoch:{0}, d_loss:{1}, g_loss: {2} '.format(epoch, d_loss_value, g_loss_value))\n",
    "                \n",
    "                # visualize loss\n",
    "                if epoch % self.epoch_saveMetrics == 0:\n",
    "                    save_metrics(model_name, self.losses, epoch)\n",
    "\n",
    "\n",
    "                # save model parameters \n",
    "                if epoch % self.epoch_saveParamter == 0:\n",
    "                    dir_path = \"model_\" + model_name\n",
    "                    if not os.path.isdir(dir_path):\n",
    "                        os.makedirs(dir_path)\n",
    "\n",
    "                    saver.save(sess, dir_path + \"/\" + str(epoch) + \".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init IRASUTOYA\n",
      "1839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujitoko/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:83: DeprecationWarning: PyUnicode_AsEncodedObject() is deprecated; use PyUnicode_AsEncodedString() to encode from str to bytes or PyCodec_Encode() for generic encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, d_loss:11.849939346313477, g_loss: 1.1982710361480713 \n",
      "epoch:1, d_loss:8.05084228515625, g_loss: 2.2720508575439453 \n",
      "epoch:2, d_loss:1.7727553844451904, g_loss: 8.402022361755371 \n",
      "epoch:3, d_loss:0.8320382833480835, g_loss: 3.9843409061431885 \n",
      "epoch:4, d_loss:1.8430705070495605, g_loss: 6.611001968383789 \n",
      "epoch:5, d_loss:1.4187909364700317, g_loss: 4.196351051330566 \n",
      "epoch:6, d_loss:1.6580487489700317, g_loss: 6.553448677062988 \n",
      "epoch:7, d_loss:1.8891054391860962, g_loss: 7.8075361251831055 \n",
      "epoch:8, d_loss:0.8485139608383179, g_loss: 4.044981002807617 \n",
      "epoch:9, d_loss:1.0161645412445068, g_loss: 4.57197380065918 \n",
      "epoch:10, d_loss:0.9562703967094421, g_loss: 7.07552433013916 \n",
      "epoch:11, d_loss:1.3553012609481812, g_loss: 7.292609214782715 \n",
      "epoch:12, d_loss:1.190163493156433, g_loss: 3.8127212524414062 \n",
      "epoch:13, d_loss:0.8329629898071289, g_loss: 4.80957555770874 \n",
      "epoch:14, d_loss:1.0682339668273926, g_loss: 7.805095672607422 \n",
      "epoch:15, d_loss:0.7376106977462769, g_loss: 5.713585376739502 \n",
      "epoch:16, d_loss:1.0351992845535278, g_loss: 5.2844977378845215 \n",
      "epoch:17, d_loss:1.0297741889953613, g_loss: 6.358570098876953 \n",
      "epoch:18, d_loss:1.0398471355438232, g_loss: 7.589082717895508 \n",
      "epoch:19, d_loss:0.549363374710083, g_loss: 6.4425835609436035 \n",
      "epoch:20, d_loss:0.7986409664154053, g_loss: 6.436317443847656 \n",
      "epoch:21, d_loss:0.7644792199134827, g_loss: 6.506105899810791 \n",
      "epoch:22, d_loss:1.0613186359405518, g_loss: 6.219761848449707 \n",
      "epoch:23, d_loss:0.7590887546539307, g_loss: 5.3838019371032715 \n",
      "epoch:24, d_loss:1.0282284021377563, g_loss: 6.053535461425781 \n",
      "epoch:25, d_loss:0.7028596997261047, g_loss: 6.552364826202393 \n",
      "epoch:26, d_loss:0.910871148109436, g_loss: 7.175019264221191 \n",
      "epoch:27, d_loss:0.7138835191726685, g_loss: 6.485898494720459 \n",
      "epoch:28, d_loss:1.0173195600509644, g_loss: 6.389425277709961 \n",
      "epoch:29, d_loss:0.7025772929191589, g_loss: 7.216675758361816 \n",
      "epoch:30, d_loss:1.0868780612945557, g_loss: 7.554238319396973 \n",
      "epoch:31, d_loss:0.6078919172286987, g_loss: 6.7281999588012695 \n",
      "epoch:32, d_loss:0.7457890510559082, g_loss: 7.210541248321533 \n",
      "epoch:33, d_loss:1.0765517950057983, g_loss: 6.567718029022217 \n",
      "epoch:34, d_loss:0.7424376010894775, g_loss: 7.363961696624756 \n",
      "epoch:35, d_loss:0.7034372091293335, g_loss: 6.678906440734863 \n",
      "epoch:36, d_loss:0.6551814675331116, g_loss: 6.8021440505981445 \n",
      "epoch:37, d_loss:0.9945786595344543, g_loss: 7.562350749969482 \n",
      "epoch:38, d_loss:0.9017912149429321, g_loss: 5.71136474609375 \n",
      "epoch:39, d_loss:0.8528582453727722, g_loss: 7.972023963928223 \n",
      "epoch:40, d_loss:0.8916285037994385, g_loss: 5.192530155181885 \n",
      "epoch:41, d_loss:0.7558854818344116, g_loss: 9.256733894348145 \n",
      "epoch:42, d_loss:0.7883685231208801, g_loss: 8.914997100830078 \n",
      "epoch:43, d_loss:1.0004924535751343, g_loss: 8.823805809020996 \n",
      "epoch:44, d_loss:0.513555645942688, g_loss: 6.9907989501953125 \n",
      "epoch:45, d_loss:0.7847377061843872, g_loss: 6.594494342803955 \n",
      "epoch:46, d_loss:0.5445235371589661, g_loss: 9.049344062805176 \n",
      "epoch:47, d_loss:0.8631093502044678, g_loss: 7.4217209815979 \n",
      "epoch:48, d_loss:0.9877034425735474, g_loss: 5.891897678375244 \n",
      "epoch:49, d_loss:0.6233199834823608, g_loss: 8.94726848602295 \n",
      "epoch:50, d_loss:0.5933888554573059, g_loss: 8.464452743530273 \n",
      "epoch:51, d_loss:0.7560522556304932, g_loss: 7.519235610961914 \n",
      "epoch:52, d_loss:0.4722764492034912, g_loss: 8.117473602294922 \n",
      "epoch:53, d_loss:0.46615809202194214, g_loss: 8.275912284851074 \n",
      "epoch:54, d_loss:0.636030375957489, g_loss: 7.758534908294678 \n",
      "epoch:55, d_loss:0.8410217761993408, g_loss: 7.753457069396973 \n",
      "epoch:56, d_loss:0.6406707763671875, g_loss: 6.35929536819458 \n",
      "epoch:57, d_loss:0.5668478608131409, g_loss: 6.939166069030762 \n",
      "epoch:58, d_loss:0.6936540603637695, g_loss: 8.073899269104004 \n",
      "epoch:59, d_loss:0.645322322845459, g_loss: 7.514026641845703 \n",
      "epoch:60, d_loss:0.6229569315910339, g_loss: 6.865265846252441 \n",
      "epoch:61, d_loss:0.7950789332389832, g_loss: 8.813047409057617 \n",
      "epoch:62, d_loss:0.9136002063751221, g_loss: 7.397642135620117 \n",
      "epoch:63, d_loss:0.6591736674308777, g_loss: 5.520761489868164 \n",
      "epoch:64, d_loss:0.9016573429107666, g_loss: 7.531156063079834 \n",
      "epoch:65, d_loss:0.6366146802902222, g_loss: 7.68084716796875 \n",
      "epoch:66, d_loss:0.5092861652374268, g_loss: 8.350018501281738 \n",
      "epoch:67, d_loss:0.7261073589324951, g_loss: 9.180319786071777 \n",
      "epoch:68, d_loss:0.6618324518203735, g_loss: 8.672186851501465 \n",
      "epoch:69, d_loss:0.5227382779121399, g_loss: 7.07932710647583 \n",
      "epoch:70, d_loss:0.731857419013977, g_loss: 6.8410420417785645 \n",
      "epoch:71, d_loss:0.7021133899688721, g_loss: 7.321121692657471 \n",
      "epoch:72, d_loss:0.7335937023162842, g_loss: 7.351851940155029 \n",
      "epoch:73, d_loss:0.598538339138031, g_loss: 6.216719627380371 \n",
      "epoch:74, d_loss:0.46736830472946167, g_loss: 7.223107814788818 \n",
      "epoch:75, d_loss:0.3596397042274475, g_loss: 7.115118503570557 \n",
      "epoch:76, d_loss:0.5282813310623169, g_loss: 5.655582904815674 \n",
      "epoch:77, d_loss:0.6804592609405518, g_loss: 6.859618186950684 \n",
      "epoch:78, d_loss:0.3498939275741577, g_loss: 7.6576361656188965 \n",
      "epoch:79, d_loss:0.6580947041511536, g_loss: 9.408611297607422 \n",
      "epoch:80, d_loss:0.5355997681617737, g_loss: 7.507277488708496 \n",
      "epoch:81, d_loss:0.5042005777359009, g_loss: 7.5099406242370605 \n",
      "epoch:82, d_loss:0.5150923728942871, g_loss: 7.379471778869629 \n",
      "epoch:83, d_loss:0.4890890121459961, g_loss: 10.384588241577148 \n",
      "epoch:84, d_loss:0.8179056644439697, g_loss: 6.158571243286133 \n",
      "epoch:85, d_loss:0.3478524684906006, g_loss: 6.766773700714111 \n",
      "epoch:86, d_loss:0.51544189453125, g_loss: 2.6388661861419678 \n",
      "epoch:87, d_loss:1.6718523502349854, g_loss: 7.982924461364746 \n",
      "epoch:88, d_loss:0.6362811326980591, g_loss: 3.7426791191101074 \n",
      "epoch:89, d_loss:1.5422163009643555, g_loss: 10.438858032226562 \n",
      "epoch:90, d_loss:0.8385311961174011, g_loss: 7.603946685791016 \n",
      "epoch:91, d_loss:0.7518320679664612, g_loss: 6.1298136711120605 \n",
      "epoch:92, d_loss:1.0278853178024292, g_loss: 7.716383457183838 \n",
      "epoch:93, d_loss:0.9005646109580994, g_loss: 7.45208740234375 \n",
      "epoch:94, d_loss:0.5853209495544434, g_loss: 3.428232431411743 \n",
      "epoch:95, d_loss:0.20143291354179382, g_loss: 10.298171997070312 \n",
      "epoch:96, d_loss:0.5267351269721985, g_loss: 5.560478687286377 \n",
      "epoch:97, d_loss:0.4636250436306, g_loss: 5.807178020477295 \n",
      "epoch:98, d_loss:0.7366599440574646, g_loss: 7.654808044433594 \n",
      "epoch:99, d_loss:0.5527811646461487, g_loss: 7.449234485626221 \n",
      "epoch:100, d_loss:0.27883872389793396, g_loss: 7.379894733428955 \n",
      "epoch:101, d_loss:0.654500424861908, g_loss: 7.117832183837891 \n",
      "epoch:102, d_loss:0.597625195980072, g_loss: 5.9961256980896 \n",
      "epoch:103, d_loss:0.7387714385986328, g_loss: 5.873337268829346 \n",
      "epoch:104, d_loss:0.509967029094696, g_loss: 5.982579708099365 \n",
      "epoch:105, d_loss:0.6656193733215332, g_loss: 7.339162826538086 \n",
      "epoch:106, d_loss:0.3959670960903168, g_loss: 5.145956993103027 \n",
      "epoch:107, d_loss:0.5427752137184143, g_loss: 5.740743637084961 \n",
      "epoch:108, d_loss:0.3176177442073822, g_loss: 8.064584732055664 \n",
      "epoch:109, d_loss:0.325615793466568, g_loss: 7.798515796661377 \n",
      "epoch:110, d_loss:0.6413057446479797, g_loss: 6.96749210357666 \n",
      "epoch:111, d_loss:0.6125268340110779, g_loss: 5.865518569946289 \n",
      "epoch:112, d_loss:0.6007680892944336, g_loss: 6.406796932220459 \n",
      "epoch:113, d_loss:0.5775284767150879, g_loss: 7.556465148925781 \n",
      "epoch:114, d_loss:0.25902581214904785, g_loss: 6.605230808258057 \n",
      "epoch:115, d_loss:0.4601895809173584, g_loss: 6.342076301574707 \n",
      "epoch:116, d_loss:0.37459245324134827, g_loss: 6.648412227630615 \n",
      "epoch:117, d_loss:0.573184072971344, g_loss: 6.059542655944824 \n",
      "epoch:118, d_loss:0.46679002046585083, g_loss: 6.0385284423828125 \n",
      "epoch:119, d_loss:0.6178209781646729, g_loss: 6.178517818450928 \n",
      "epoch:120, d_loss:0.55645751953125, g_loss: 6.972282409667969 \n",
      "epoch:121, d_loss:0.44569170475006104, g_loss: 7.76093864440918 \n",
      "epoch:122, d_loss:0.355049192905426, g_loss: 6.845721244812012 \n",
      "epoch:123, d_loss:0.4729440212249756, g_loss: 6.340789318084717 \n",
      "epoch:124, d_loss:0.3993377685546875, g_loss: 6.196156024932861 \n",
      "epoch:125, d_loss:0.6507083177566528, g_loss: 7.082831382751465 \n",
      "epoch:126, d_loss:0.5344972610473633, g_loss: 6.681519508361816 \n",
      "epoch:127, d_loss:0.5855103135108948, g_loss: 6.122467517852783 \n",
      "epoch:128, d_loss:0.44094279408454895, g_loss: 7.813012599945068 \n",
      "epoch:129, d_loss:0.3687503933906555, g_loss: 9.716330528259277 \n",
      "epoch:130, d_loss:0.47162124514579773, g_loss: 6.6068806648254395 \n",
      "epoch:131, d_loss:0.3868277072906494, g_loss: 5.8055925369262695 \n",
      "epoch:132, d_loss:0.5474410057067871, g_loss: 7.890571594238281 \n",
      "epoch:133, d_loss:0.49775993824005127, g_loss: 9.709757804870605 \n",
      "epoch:134, d_loss:0.5189658403396606, g_loss: 9.210103988647461 \n",
      "epoch:135, d_loss:0.495595782995224, g_loss: 7.061344623565674 \n",
      "epoch:136, d_loss:0.4623299539089203, g_loss: 4.319398403167725 \n",
      "epoch:137, d_loss:0.34969136118888855, g_loss: 8.027838706970215 \n",
      "epoch:138, d_loss:0.6138109564781189, g_loss: 8.784958839416504 \n",
      "epoch:139, d_loss:0.4072294235229492, g_loss: 7.123128890991211 \n",
      "epoch:140, d_loss:0.5864865183830261, g_loss: 7.044430255889893 \n",
      "epoch:141, d_loss:0.8077377676963806, g_loss: 8.140003204345703 \n",
      "epoch:142, d_loss:0.6141905188560486, g_loss: 6.314638137817383 \n",
      "epoch:143, d_loss:0.49495038390159607, g_loss: 6.243907928466797 \n",
      "epoch:144, d_loss:0.7861828804016113, g_loss: 6.44801139831543 \n",
      "epoch:145, d_loss:0.695193886756897, g_loss: 6.536118984222412 \n",
      "epoch:146, d_loss:0.5020045042037964, g_loss: 7.126201629638672 \n",
      "epoch:147, d_loss:0.6018720865249634, g_loss: 7.765888690948486 \n",
      "epoch:148, d_loss:0.2751745581626892, g_loss: 6.7248382568359375 \n",
      "epoch:149, d_loss:0.47762182354927063, g_loss: 5.396763801574707 \n",
      "epoch:150, d_loss:0.3374328017234802, g_loss: 5.086094856262207 \n",
      "epoch:151, d_loss:0.5436168909072876, g_loss: 7.206258296966553 \n",
      "epoch:152, d_loss:0.5677230954170227, g_loss: 6.026543617248535 \n",
      "epoch:153, d_loss:0.594537615776062, g_loss: 5.713840484619141 \n",
      "epoch:154, d_loss:0.5525280237197876, g_loss: 7.066518306732178 \n",
      "epoch:155, d_loss:0.5563291907310486, g_loss: 6.496542453765869 \n",
      "epoch:156, d_loss:0.47421079874038696, g_loss: 5.784975528717041 \n",
      "epoch:157, d_loss:0.5021495819091797, g_loss: 7.30808162689209 \n",
      "epoch:158, d_loss:0.5616698265075684, g_loss: 6.118310451507568 \n",
      "epoch:159, d_loss:0.4658040404319763, g_loss: 7.067334175109863 \n",
      "epoch:160, d_loss:0.3335018754005432, g_loss: 7.962772846221924 \n",
      "epoch:161, d_loss:0.35131561756134033, g_loss: 8.269010543823242 \n",
      "epoch:162, d_loss:0.4887680411338806, g_loss: 8.068524360656738 \n",
      "epoch:163, d_loss:0.570367693901062, g_loss: 8.45175838470459 \n",
      "epoch:164, d_loss:0.4874473810195923, g_loss: 7.501838684082031 \n",
      "epoch:165, d_loss:0.5411009192466736, g_loss: 7.165859222412109 \n",
      "epoch:166, d_loss:0.5946434140205383, g_loss: 8.19157600402832 \n",
      "epoch:167, d_loss:0.3322080373764038, g_loss: 7.79201602935791 \n",
      "epoch:168, d_loss:0.843996524810791, g_loss: 5.921334266662598 \n",
      "epoch:169, d_loss:0.39608946442604065, g_loss: 5.348713397979736 \n",
      "epoch:170, d_loss:0.5458022952079773, g_loss: 7.962900161743164 \n",
      "epoch:171, d_loss:0.4191272258758545, g_loss: 6.460912227630615 \n",
      "epoch:172, d_loss:0.38874128460884094, g_loss: 5.818986892700195 \n",
      "epoch:173, d_loss:0.4159168601036072, g_loss: 6.758432388305664 \n",
      "epoch:174, d_loss:0.3914126455783844, g_loss: 7.630388259887695 \n",
      "epoch:175, d_loss:0.6920956373214722, g_loss: 6.965645790100098 \n",
      "epoch:176, d_loss:0.47290748357772827, g_loss: 6.884006023406982 \n",
      "epoch:177, d_loss:0.575326681137085, g_loss: 6.20613956451416 \n",
      "epoch:178, d_loss:0.40401408076286316, g_loss: 6.591745853424072 \n",
      "epoch:179, d_loss:0.16740113496780396, g_loss: 8.14362907409668 \n",
      "epoch:180, d_loss:0.32629454135894775, g_loss: 5.988219261169434 \n",
      "epoch:181, d_loss:0.4890103042125702, g_loss: 5.360745906829834 \n",
      "epoch:182, d_loss:0.27921807765960693, g_loss: 8.43070125579834 \n",
      "epoch:183, d_loss:0.43594688177108765, g_loss: 8.042692184448242 \n",
      "epoch:184, d_loss:0.33924782276153564, g_loss: 7.075020790100098 \n",
      "epoch:185, d_loss:0.4106403589248657, g_loss: 6.730869293212891 \n",
      "epoch:186, d_loss:0.48807471990585327, g_loss: 7.569710731506348 \n",
      "epoch:187, d_loss:0.5600171089172363, g_loss: 5.9316911697387695 \n",
      "epoch:188, d_loss:0.517052412033081, g_loss: 7.522173881530762 \n",
      "epoch:189, d_loss:0.8946382999420166, g_loss: 7.654581069946289 \n",
      "epoch:190, d_loss:0.9678982496261597, g_loss: 5.658012390136719 \n",
      "epoch:191, d_loss:0.5355453491210938, g_loss: 7.786338806152344 \n",
      "epoch:192, d_loss:0.6137121915817261, g_loss: 8.018975257873535 \n",
      "epoch:193, d_loss:0.44403597712516785, g_loss: 6.865320205688477 \n",
      "epoch:194, d_loss:0.6005516052246094, g_loss: 4.459086894989014 \n",
      "epoch:195, d_loss:0.26737499237060547, g_loss: 9.06400203704834 \n",
      "epoch:196, d_loss:0.281637966632843, g_loss: 11.915546417236328 \n",
      "epoch:197, d_loss:0.2729666233062744, g_loss: 7.901958465576172 \n",
      "epoch:198, d_loss:0.5579914450645447, g_loss: 7.1624250411987305 \n",
      "epoch:199, d_loss:0.40709370374679565, g_loss: 6.394623279571533 \n"
     ]
    }
   ],
   "source": [
    "gan = GAN()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
