{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN implementation by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_metrics(metrics, epoch=None):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"dis_loss\"], label=\"discriminative loss\", color=\"b\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(\"metrics\", \"dloss\" + str(epoch) + \".png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"gen_loss\"], label=\"generative loss\", color=\"r\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(\"metrics\", \"g_loss\" + str(epoch) + \".png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"gen_loss\"], label=\"generative loss\", color=\"r\")\n",
    "    plt.plot(metrics[\"dis_loss\"], label=\"discriminative loss\", color=\"b\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(\"metrics\", \"both_loss\" + str(epoch) + \".png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save generated images\n",
    "def save_imgs(images, plot_dim=(5,12), size=(12,5), epoch=None):\n",
    "    examples = plot_dim[0]*plot_dim[1]\n",
    "\n",
    "    # 表示\n",
    "    fig = plt.figure(figsize=size)\n",
    "    for i in range(examples):\n",
    "        plt.subplot(plot_dim[0], plot_dim[1], i+1)\n",
    "        img = images[i, :]\n",
    "        img = img.reshape((96, 96, 3))\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.savefig(os.path.join(\"generated_figures\", str(epoch) + \".png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.z_dim = 100\n",
    "        self.s_size = 6\n",
    "        self.depths = [1024, 512, 256, 128, 3]\n",
    "        \n",
    "    def __call__(self, inputs, training=False):\n",
    "        with tf.variable_scope('g', reuse=self.reuse):\n",
    "            # reshape from inputs\n",
    "            outputs = tf.reshape(inputs, [-1, self.z_dim])\n",
    "            \n",
    "            with tf.variable_scope('reshape'):\n",
    "                outputs = tf.layers.dense(outputs, self.depths[0] * self.s_size * self.s_size)\n",
    "                outputs = tf.reshape(outputs, [-1, self.s_size, self.s_size, self.depths[0]])\n",
    "                outputs = tf.nn.relu(tf.layers.batch_normalization(outputs, training=training), name='outputs')\n",
    "            # deconvolution (transpose of convolution) x 4\n",
    "            with tf.variable_scope('deconv1'):\n",
    "                outputs = tf.layers.conv2d_transpose(outputs, self.depths[1], [5, 5], strides=(2, 2), padding='SAME')\n",
    "                outputs = tf.nn.relu(tf.layers.batch_normalization(outputs, training=training), name='outputs')\n",
    "            with tf.variable_scope('deconv2'):\n",
    "                outputs = tf.layers.conv2d_transpose(outputs, self.depths[2], [5, 5], strides=(2, 2), padding='SAME')\n",
    "                outputs = tf.nn.relu(tf.layers.batch_normalization(outputs, training=training), name='outputs')\n",
    "            with tf.variable_scope('deconv3'):\n",
    "                outputs = tf.layers.conv2d_transpose(outputs, self.depths[3], [5, 5], strides=(2, 2), padding='SAME')\n",
    "                outputs = tf.nn.relu(tf.layers.batch_normalization(outputs, training=training), name='outputs')\n",
    "            with tf.variable_scope('deconv4'):\n",
    "                outputs = tf.layers.conv2d_transpose(outputs, self.depths[4], [5, 5], strides=(2, 2), padding='SAME')\n",
    "            # output images\n",
    "            with tf.variable_scope('tanh'):\n",
    "                outputs = tf.sigmoid(outputs)\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='g')\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, batch_size=128):\n",
    "        self.reuse = False\n",
    "        self.X_dim = 96*96*3\n",
    "        self.s_size = 6\n",
    "        self.depths = [3, 64, 128, 256, 512]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __call__(self, inputs, training=False, name=''):\n",
    "        def leaky_relu(x, leak=0.2, name='outputs'):\n",
    "            return tf.maximum(x, x * leak, name=name)\n",
    "\n",
    "        with tf.name_scope('d' + name), tf.variable_scope('d', reuse=self.reuse):\n",
    "            # convolution x 4\n",
    "            outputs = tf.reshape(inputs, [-1, 96, 96, 3])\n",
    "            with tf.variable_scope('conv1'):\n",
    "                outputs = tf.layers.conv2d(outputs, self.depths[1], [5, 5], strides=(2, 2), padding='SAME')\n",
    "                outputs = leaky_relu(tf.layers.batch_normalization(outputs, training=training), name='outputs')\n",
    "            with tf.variable_scope('conv2'):\n",
    "                outputs = tf.layers.conv2d(outputs, self.depths[2], [5, 5], strides=(2, 2), padding='SAME')\n",
    "                outputs = leaky_relu(tf.layers.batch_normalization(outputs, training=training), name='outputs')\n",
    "            with tf.variable_scope('conv3'):\n",
    "                outputs = tf.layers.conv2d(outputs, self.depths[3], [5, 5], strides=(2, 2), padding='SAME')\n",
    "                outputs = leaky_relu(tf.layers.batch_normalization(outputs, training=training), name='outputs')\n",
    "            with tf.variable_scope('conv4'):\n",
    "                outputs = tf.layers.conv2d(outputs, self.depths[4], [5, 5], strides=(2, 2), padding='SAME')\n",
    "                outputs = leaky_relu(tf.layers.batch_normalization(outputs, training=training), name='outputs')\n",
    "            with tf.variable_scope('classify'):\n",
    "                outputs = tf.layers.dense(outputs, 1, name='outputs')\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='d')\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "    def __init__(self, batch_size=128):\n",
    "        self.batch_size = batch_size\n",
    "        self.g = Generator()\n",
    "        self.d = Discriminator()\n",
    "        self.z_dim = 100\n",
    "\n",
    "    def d_loss_calc(self, g_outputs, t_outputs, gradient_penalty):\n",
    "        lambda_ = 10\n",
    "\n",
    "        d_loss_train = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=t_outputs, labels=tf.ones_like(t_outputs)))\n",
    "        d_loss_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=g_outputs, labels=tf.zeros_like(g_outputs)))\n",
    "        loss = d_loss_train + d_loss_gen\n",
    "        loss = loss + lambda_*gradient_penalty\n",
    "        return loss\n",
    "\n",
    "    def g_loss_calc(self, g_outputs):\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=g_outputs, labels=tf.ones_like(g_outputs)))\n",
    "        return loss\n",
    "\n",
    "    def loss(self, traindata, perturbed_minibatch):\n",
    "        z = tf.random_uniform([self.batch_size, self.z_dim], minval=-1.0, maxval=1.0)\n",
    "        generated = self.g(z, training=True)\n",
    "        g_outputs = self.d(generated, training=True, name='g')\n",
    "        t_outputs = self.d(traindata, training=True, name='t')\n",
    "        \n",
    "        alpha = tf.random_uniform(shape=[self.batch_size,1], minval=0., maxval=1.)\n",
    "\n",
    "        diff = perturbed_minibatch - traindata\n",
    "        interpolates = traindata + (alpha*diff)\n",
    "        gradients = tf.gradients(self.d(interpolates), [interpolates])[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "        gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "\n",
    "        # add each losses to collection\n",
    "        tf.add_to_collection(\n",
    "            'g_losses',\n",
    "            self.g_loss_calc(g_outputs))\n",
    "\n",
    "        tf.add_to_collection(\n",
    "            'd_losses',\n",
    "            self.d_loss_calc(g_outputs, t_outputs, gradient_penalty)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            self.g: tf.add_n(tf.get_collection('g_losses'), name='total_g_loss'),\n",
    "            self.d: tf.add_n(tf.get_collection('d_losses'), name='total_d_loss'),\n",
    "        }\n",
    "\n",
    "    def d_train(self, losses, learning_rate=1e-4, beta1=0.5, beta2=0.9):\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2)\n",
    "        d_opt_op = d_opt.minimize(losses[self.d], var_list=self.d.variables)\n",
    "        return d_opt_op\n",
    "\n",
    "    def g_train(self, losses, learning_rate=1e-4, beta1=0.5, beta2=0.9):\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2)\n",
    "        g_opt_op = g_opt.minimize(losses[self.g], var_list=self.g.variables)\n",
    "        return g_opt_op\n",
    "\n",
    "    def sample_images(self, row=5, col=12, inputs=None, epoch=None):\n",
    "        images = self.g(inputs, training=True)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load(\"irasutoya_face_1813x96x96x3_jpg.npy\")\n",
    "X_train = X_train/255\n",
    "X_dim = 96*96*3\n",
    "z_dim = 100\n",
    "batch_size = 128\n",
    "epochs = 500000\n",
    "display_epoch = 100\n",
    "param_save_epoch = 10000\n",
    "loss = {\"dis_loss\":[], \"gen_loss\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_noise = tf.placeholder(tf.float32, [None, z_dim])\n",
    "noise_check = np.random.uniform(-1, 1, size=[60, z_dim]).astype(np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, X_dim])\n",
    "X_p = tf.placeholder(tf.float32, [None, X_dim])\n",
    "\n",
    "dcgan = DCGAN()\n",
    "losses = dcgan.loss(traindata=X, perturbed_minibatch=X_p)\n",
    "d_train_op = dcgan.d_train(losses)\n",
    "g_train_op = dcgan.g_train(losses)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#%debug\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % display_epoch == 0:\n",
    "            imgs_gen = dcgan.sample_images(inputs=noise_check).eval()\n",
    "            print(\"saving images\")\n",
    "            save_imgs(imgs_gen, epoch=epoch)\n",
    "        for _ in range(10):\n",
    "\n",
    "            # 訓練データを抜粋\n",
    "            rand_index = np.random.randint(0, X_train.shape[0], size=batch_size)\n",
    "            X_mb = X_train[rand_index, :].astype(np.float32)\n",
    "            X_mb = X_mb.reshape([-1, X_dim])\n",
    "\n",
    "            # x + delta vector\n",
    "            X_mb_p = X_mb + 0.5*X_mb.std()*np.random.random(X_mb.shape)\n",
    "\n",
    "            _, d_loss_value = sess.run([d_train_op, losses[dcgan.d]], feed_dict={X: X_mb, X_p:X_mb_p})\n",
    "        _, g_loss_value, = sess.run([g_train_op, losses[dcgan.g]], feed_dict={X: X_mb, X_p:X_mb_p})\n",
    "\n",
    "        # 結果をappend\n",
    "        loss[\"dis_loss\"].append(d_loss_value)\n",
    "        loss[\"gen_loss\"].append(g_loss_value)\n",
    "        print(\"epoch:\" + str(epoch))\n",
    "        # グラフの描画（余裕があったら）\n",
    "        if epoch % display_epoch == 0:\n",
    "            save_metrics(loss, epoch)\n",
    "\n",
    "        if epoch % param_save_epoch == 0:\n",
    "            saver.save(sess, \"./model/dcgan_model\" + str(epoch) + \".ckpt\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
