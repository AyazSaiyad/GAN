{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from model import *\n",
    "from utility import *\n",
    "\n",
    "model_name = \"DRAGAN_SRResNet_for_IRASUTOYA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.g_bn0 = BatchNormalization(name = 'g_bn0')\n",
    "        self.g_bn1 = BatchNormalization(name = 'g_bn1')\n",
    "        \n",
    "        self.num_res_blocks = 16\n",
    "        self.num_pixel_CNN_blocks = 3\n",
    "        \n",
    "        self.res_bns = []\n",
    "        for i in range(int(self.num_res_blocks)):\n",
    "            self.res_bns.append(BatchNormalization(name = \"res_%d\" % (2*i)))\n",
    "            self.res_bns.append(BatchNormalization(name = \"res_%d\" % (2*i+1)))\n",
    "        \n",
    "        self.ps_bns = []\n",
    "        for i in range(int(self.num_pixel_CNN_blocks)):\n",
    "            self.ps_bns.append(BatchNormalization(name = \"ps_%d\" % i))\n",
    "\n",
    "    def __call__(self, z):\n",
    "        with tf.variable_scope('g', reuse=self.reuse):\n",
    "            \n",
    "            # reshape from inputs\n",
    "            with tf.variable_scope('fc0'):\n",
    "                #z0 = tf.reshape(z, [-1, self.z_dim])\n",
    "                fc0 = full_connection_layer(z, 64*16*16, name=\"fc0\")\n",
    "                fc0 = self.g_bn0(fc0)\n",
    "                fc0 = tf.nn.relu(fc0)\n",
    "                fc0 = tf.reshape(fc0, [-1,16,16,64])\n",
    "\n",
    "            assert fc0.get_shape().as_list()[1:] == [16,16,64]\n",
    "            \n",
    "            layers = []\n",
    "            layers.append(fc0)\n",
    "            \n",
    "            for i in range(int(self.num_res_blocks)):\n",
    "                with tf.variable_scope('res_%d' % (i+1)):\n",
    "                    res = conv2d_layer(layers[-1], 64, kernel_size=3, strides=1, name=\"g_conv_res_%d\" % (2*i))\n",
    "                    res = self.res_bns[2*i](res)\n",
    "                    res = tf.nn.relu(res)\n",
    "\n",
    "                    res = conv2d_layer(res, 64, kernel_size=3, strides=1, name=\"g_conv_res_%d\" % (2*i+1))\n",
    "                    res = self.res_bns[2*i+1](res)\n",
    "                    res = layers[-1] + res\n",
    "                    layers.append(res)                    \n",
    "\n",
    "            assert layers[-1].get_shape().as_list()[1:] == [16,16,64]\n",
    "            \n",
    "            with tf.variable_scope('conv17'):\n",
    "                conv17 = conv2d_layer(layers[-1], 64, kernel_size=3, strides=1, name=\"g_conv_17\")\n",
    "                conv17 = self.g_bn1(conv17)\n",
    "                conv17 = tf.nn.relu(conv17)\n",
    "                conv17 = layers[0] + conv17\n",
    "                layers.append(conv17)\n",
    "\n",
    "            assert layers[-1].get_shape().as_list()[1:] == [16, 16, 64]\n",
    "\n",
    "            for i in range(int(self.num_pixel_CNN_blocks)):\n",
    "                with tf.variable_scope('pixel_CNN_%d' % (i+1)):\n",
    "                    ps = conv2d_layer(layers[-1], 256, kernel_size=3, strides=1, name=\"g_conv_ps_%d\" % (i))\n",
    "                    ps = pixel_shuffle_layer(ps, 2, 64)\n",
    "                    ps = self.ps_bns[i](ps)\n",
    "                    ps = tf.nn.relu(ps)\n",
    "                    layers.append(ps)\n",
    "\n",
    "            assert layers[-1].get_shape().as_list()[1:] == [128, 128, 64]\n",
    "            \n",
    "            with tf.variable_scope('output'):\n",
    "                output = conv2d_layer(layers[-1], 3, kernel_size=9, strides=1, name=\"output\")\n",
    "                output = tf.nn.sigmoid(output)\n",
    "\n",
    "            assert output.get_shape().as_list()[1:] == [128, 128, 3]            \n",
    "            output = tf.reshape(output, [-1, 128*128*3])\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='g')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.d_bn0 = BatchNormalization(name=\"d_bn0\")\n",
    "        self.d_bn1 = BatchNormalization(name=\"d_bn1\")\n",
    "        self.d_bn2 = BatchNormalization(name=\"d_bn2\")\n",
    "        self.d_bn3 = BatchNormalization(name=\"d_bn3\")\n",
    "        self.d_bn4 = BatchNormalization(name=\"d_bn4\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        with tf.variable_scope('d', reuse=self.reuse):\n",
    "           \n",
    "            # convolution x 4\n",
    "            x = tf.reshape(x, [-1, 128, 128, 3])\n",
    "            with tf.variable_scope('conv1'):\n",
    "                conv1 = conv2d_layer(x, 32, kernel_size=4, strides=2, name=\"conv1\")\n",
    "                conv1 = lrelu(conv1, leak=0.2)\n",
    "\n",
    "            with tf.variable_scope('res1'):                \n",
    "                res1 = conv2d_layer(conv1, 32, kernel_size=3, strides=1, name=\"res1_conv0\")\n",
    "                res1 = lrelu(res1, leak=0.2)\n",
    "                res1 = conv2d_layer(res1, 32, kernel_size=3, strides=1, name=\"res1_conv1\")\n",
    "                res1 = res1 + conv1\n",
    "                res1 = lrelu(res1, leak=0.2)\n",
    "                \n",
    "            with tf.variable_scope('res2'):\n",
    "                res2 = conv2d_layer(res1, 32, kernel_size=3, strides=1, name=\"res2_conv0\")\n",
    "                res2 = lrelu(res2, leak=0.2)\n",
    "                res2 = conv2d_layer(res2, 32, kernel_size=3, strides=1, name=\"res2_conv1\")\n",
    "                res2 = res2 + res1\n",
    "                res2 = lrelu(res2, leak=0.2)\n",
    "\n",
    "            with tf.variable_scope('conv2'):\n",
    "                conv2 = conv2d_layer(res2, 64, kernel_size=4, strides=2, name=\"conv2\")\n",
    "                conv2 = lrelu(conv2, leak=0.2)\n",
    "\n",
    "            with tf.variable_scope('res3'):\n",
    "                res3 = conv2d_layer(conv2, 64, kernel_size=3, strides=1, name=\"res3_conv0\")\n",
    "                res3 = lrelu(res3, leak=0.2)\n",
    "                res3 = conv2d_layer(res3, 64, kernel_size=3, strides=1, name=\"res3_conv1\")\n",
    "                res3 = res3 + conv2\n",
    "                res3 = lrelu(res3, leak=0.2)            \n",
    "\n",
    "            with tf.variable_scope('res4'):\n",
    "                res4 = conv2d_layer(res3, 64, kernel_size=3, strides=1, name=\"res4_conv0\")\n",
    "                res4 = lrelu(res4, leak=0.2)\n",
    "                res4 = conv2d_layer(res4, 64, kernel_size=3, strides=1, name=\"res4_conv1\")\n",
    "                res4 = res4 + res3\n",
    "                res4 = lrelu(res4, leak=0.2)                    \n",
    "                \n",
    "            with tf.variable_scope('conv3'):\n",
    "                conv3 = conv2d_layer(res4, 128, kernel_size=4, strides=2, name=\"conv3\")\n",
    "                conv3 = lrelu(conv3, leak=0.2)\n",
    "\n",
    "            num_res_itr = 3\n",
    "            layers = []\n",
    "            layers.append(conv3)\n",
    "            \n",
    "            depth = [128, 256, 512, 1024]\n",
    "            for i in range(int(num_res_itr)):\n",
    "                with tf.variable_scope('res_%d_1' % (i+1+4)):\n",
    "                    res = conv2d_layer(layers[-1], depth[i], kernel_size=3, strides=1, name=\"res_%d\" % (5*i+0))\n",
    "                    res = lrelu(res)\n",
    "                    res = conv2d_layer(res, depth[i], kernel_size=3, strides=1, name=\"res_%d\" % (5*i+1))\n",
    "                    res = lrelu(res)\n",
    "                    res = layers[-1] + res\n",
    "                    res = lrelu(res)\n",
    "                layers.append(res)\n",
    "\n",
    "                with tf.variable_scope('res_%d_2' % (i+1+4)):\n",
    "                    res = conv2d_layer(layers[-1], depth[i], kernel_size=3, strides=1, name=\"res_%d\" % (5*i+2))\n",
    "                    res = lrelu(res)\n",
    "                    res = conv2d_layer(res, depth[i], kernel_size=3, strides=1, name=\"res_%d\" % (5*i+3))\n",
    "                    res = lrelu(res)\n",
    "                    res = layers[-1] + res\n",
    "                    res = lrelu(res)                  \n",
    "                    \n",
    "                conv = conv2d_layer(res, depth[i+1], kernel_size=4, strides=2, name=\"res_%d\" % (5*i+4))\n",
    "                conv = lrelu(conv) \n",
    "                layers.append(conv)\n",
    "                \n",
    "            disc = full_connection_layer(layers[-1], 1, name=\"disc\")\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='d')\n",
    "\n",
    "        return disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.img_size = 128\n",
    "        self.rand_size = 100\n",
    "        self.z_size = self.rand_size\n",
    "        \n",
    "        self.epochs = 50000\n",
    "        self.epoch_saveMetrics = 200\n",
    "        self.epoch_saveSampleImg = 200\n",
    "        self.epoch_saveParamter = 5000\n",
    "        self.losses = {\"d_loss\":[], \"g_loss\":[]}\n",
    "\n",
    "        # unrolled counts\n",
    "        self.steps = 10\n",
    "\n",
    "        self.X_tr = tf.placeholder(tf.float32, shape=[None, self.img_size*self.img_size*3])\n",
    "        self.z = tf.placeholder(tf.float32, [None, self.z_size])\n",
    "        self.X_per = tf.placeholder(tf.float32, shape=[None, self.img_size*self.img_size*3])\n",
    "\n",
    "        self.g = Generator()\n",
    "        self.d = Discriminator()\n",
    "        self.Xg = self.g(self.z)\n",
    "        #self.pokemon = Pokemon()\n",
    "        self.irasutoya = IRASUTOYA()\n",
    "        self.learning_rate = tf.placeholder(\"float\", [])\n",
    "        self.count = 0\n",
    "    def loss(self):\n",
    "        self.count += 1\n",
    "        tf.summary.image(\"X_tr\", tf.reshape(self.X_tr, [-1, 128, 128, 3]))\n",
    "        tf.summary.image(\"Xg\", tf.reshape(self.Xg, [-1, 128, 128, 3]))\n",
    "        disc_tr = self.d(self.X_tr)\n",
    "        disc_gen = self.d(self.Xg)\n",
    "        self.disc_tr = disc_tr\n",
    "        self.disc_gen = disc_gen\n",
    "        \n",
    "        #tf.summary.histogram('disc_tr', disc_tr)\n",
    "        #tf.summary.histogram(\"disc_gen\", disc_gen)\n",
    "\n",
    "        lambda_gp = 10\n",
    "       \n",
    "        loss_g = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen, labels=tf.ones_like(disc_gen)))\n",
    "\n",
    "        diff = self.X_per - self.X_tr\n",
    "        alpha = tf.random_uniform(shape=[self.batch_size,1,1,1], minval=0., maxval=1.)\n",
    "        interpolates = self.X_tr + (alpha*diff)\n",
    "        disc_interplates = self.d(interpolates)\n",
    "        gradients = tf.gradients(disc_interplates, [interpolates])[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "        gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "\n",
    "        loss_d_tr = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_tr, labels=tf.ones_like(disc_tr)))\n",
    "        loss_d_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen, labels=tf.zeros_like(disc_gen)))\n",
    "        loss_d = (loss_d_tr + loss_d_gen)\n",
    "        loss_d += lambda_gp*gradient_penalty\n",
    "\n",
    "\n",
    "        tf.summary.scalar(\"loss_g\", loss_g)\n",
    "        tf.summary.scalar(\"loss_d\", loss_d)\n",
    "        return loss_g, loss_d\n",
    "\n",
    "    def train(self):\n",
    "        # Optimizer\n",
    "        d_lr = 1e-4\n",
    "        d_beta1 = 0.5\n",
    "        g_lr = 1e-4\n",
    "        g_beta1 = 0.5\n",
    "\n",
    "        self.L_g, self.L_d = self.loss()\n",
    "\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate=d_lr)\n",
    "        d_train_op = d_opt.minimize(self.L_d, var_list=self.d.variables)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=g_lr)\n",
    "        g_train_op = g_opt.minimize(self.L_g, var_list=self.g.variables)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        config = tf.ConfigProto(\n",
    "            gpu_options=tf.GPUOptions(\n",
    "                visible_device_list= \"0\"\n",
    "            )\n",
    "        )\n",
    "                \n",
    "        summary = tf.summary.merge_all()\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "            writer = tf.summary.FileWriter('cnn', sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # preparing noise vec for test\n",
    "            bs = 100\n",
    "            test_z = np.random.uniform(-1, 1, size=[bs, self.rand_size])\n",
    "\n",
    "            lr = 1e-4\n",
    "            for epoch in range(self.epochs):\n",
    "                if epoch > 2000:\n",
    "                    lr = 5e-5\n",
    "                if epoch > 4000:\n",
    "                    lr = 1e-5\n",
    "\n",
    "                # visualize generated images during training\n",
    "                if epoch % self.epoch_saveSampleImg == 0:\n",
    "                    img = sess.run(self.Xg, feed_dict={self.z: test_z})\n",
    "                    img = np.reshape(img, [-1, 128, 128, 3])\n",
    "                    img += 0.5\n",
    "                    save_imgs(model_name, img, name=str(epoch))\n",
    "\n",
    "                for step in range(self.steps):\n",
    "                    # extract images for training\n",
    "                    #rand_index = np.random.randint(0, self.dataset.shape[0], size=self.batch_size)\n",
    "                    #X_mb, Y_mb = self.dataset[rand_index, :].astype(np.float32)\n",
    "                    #X_mb = self.dtd.extract(self.batch_size, self.img_size)                   \n",
    "                    X_mb, Y_mb = self.irasutoya.extract(self.batch_size, self.img_size)\n",
    "\n",
    "                    #X_mb = self.pokemon.extract(self.batch_size, self.img_size)\n",
    "                    X_mb = np.reshape(X_mb, [-1, self.img_size*self.img_size*3])\n",
    "                    X_mb_per = X_mb + 0.5*np.std(X_mb)*np.random.random(X_mb.shape)\n",
    "                    #save_imgs(model_name, X_mb, name=str(epoch)+\"_d\")\n",
    "\n",
    "                    rand = np.random.uniform(-1, 1, size=[self.batch_size, self.rand_size])\n",
    "\n",
    "                    # train Discriminator\n",
    "                    _, d_loss_value = sess.run([d_train_op, self.L_d], feed_dict={\n",
    "                        self.X_tr: X_mb,\n",
    "                        self.z:rand,\n",
    "                        self.X_per: X_mb_per,\n",
    "                    })\n",
    "\n",
    "                # train Generator\n",
    "                disc_tr, disc_gen, _, g_loss_value, w_summary = sess.run([self.disc_tr, self.disc_gen, g_train_op, self.L_g, summary], feed_dict={\n",
    "                    self.X_tr: X_mb,\n",
    "                    self.z:rand,\n",
    "                    self.X_per: X_mb_per,\n",
    "                })\n",
    "\n",
    "                writer.add_summary(w_summary, epoch)\n",
    "\n",
    "                # append loss value for visualizing\n",
    "                self.losses[\"d_loss\"].append(np.sum(d_loss_value))\n",
    "                self.losses[\"g_loss\"].append(np.sum(g_loss_value))\n",
    "                \n",
    "                # print epoch\n",
    "                if epoch % 1 == 0:\n",
    "                    print('epoch:{0}, d_loss:{1}, g_loss: {2}, disc_tr: {3}, disc_gen: {4} '.format(epoch, d_loss_value, g_loss_value, disc_tr, disc_gen))\n",
    "                \n",
    "                # visualize loss\n",
    "                if epoch % self.epoch_saveMetrics == 0:\n",
    "                    save_metrics(model_name, self.losses, epoch)\n",
    "\n",
    "\n",
    "                # save model parameters \n",
    "                if epoch % self.epoch_saveParamter == 0:\n",
    "                    dir_path = \"model_\" + model_name\n",
    "                    if not os.path.isdir(dir_path):\n",
    "                        os.makedirs(dir_path)\n",
    "\n",
    "                    saver.save(sess, dir_path + \"/\" + str(epoch) + \".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init IRASUTOYA\n",
      "1839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujitoko/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:80: DeprecationWarning: PyUnicode_AsEncodedObject() is deprecated; use PyUnicode_AsEncodedString() to encode from str to bytes or PyCodec_Encode() for generic encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, d_loss:10.552452087402344, g_loss: 12.07174301147461, disc_tr: [[ 0.57227427]\n",
      " [ 0.57704484]], disc_gen: [[-12.01258755]\n",
      " [-12.13088799]] \n",
      "epoch:1, d_loss:9.780306816101074, g_loss: 260.17816162109375, disc_tr: [[ 36.99403  ]\n",
      " [ 14.4256382]], disc_gen: [[-260.03598022]\n",
      " [-260.32034302]] \n",
      "epoch:2, d_loss:23.921293258666992, g_loss: 8.051680924836546e-05, disc_tr: [[ 12.40630341]\n",
      " [ 12.96924591]], disc_gen: [[ 9.46440601]\n",
      " [ 9.39095116]] \n",
      "epoch:3, d_loss:11.37394905090332, g_loss: 0.7275923490524292, disc_tr: [[-0.0733645 ]\n",
      " [-0.06950997]], disc_gen: [[-0.06763718]\n",
      " [-0.06784932]] \n",
      "epoch:4, d_loss:11.410528182983398, g_loss: 0.8009722232818604, disc_tr: [[-0.25259614]\n",
      " [-0.20173454]], disc_gen: [[-0.20616636]\n",
      " [-0.20412742]] \n",
      "epoch:5, d_loss:11.387401580810547, g_loss: 0.7488482594490051, disc_tr: [[-0.10363361]\n",
      " [-0.09999483]], disc_gen: [[-0.10798794]\n",
      " [-0.10893709]] \n",
      "epoch:6, d_loss:11.363054275512695, g_loss: 0.7207698822021484, disc_tr: [[-0.00057218]\n",
      " [-0.01119529]], disc_gen: [[-0.05353763]\n",
      " [-0.05546779]] \n",
      "epoch:7, d_loss:11.35472583770752, g_loss: 0.7109261751174927, disc_tr: [[ 0.00927212]\n",
      " [ 0.05249346]], disc_gen: [[-0.03579843]\n",
      " [-0.03469616]] \n",
      "epoch:8, d_loss:11.344457626342773, g_loss: 0.7101011276245117, disc_tr: [[ 0.04517954]\n",
      " [ 0.05948729]], disc_gen: [[-0.03271975]\n",
      " [-0.03453055]] \n",
      "epoch:9, d_loss:11.342205047607422, g_loss: 0.7185258269309998, disc_tr: [[ 0.10436701]\n",
      " [-0.02178949]], disc_gen: [[-0.05207697]\n",
      " [-0.04817941]] \n",
      "epoch:10, d_loss:11.298318862915039, g_loss: 0.7317695617675781, disc_tr: [[ 0.1348732]\n",
      " [ 0.0840238]], disc_gen: [[-0.07569222]\n",
      " [-0.07592459]] \n",
      "epoch:11, d_loss:11.268665313720703, g_loss: 0.7530333995819092, disc_tr: [[ 0.1401372]\n",
      " [ 0.1288632]], disc_gen: [[-0.11831798]\n",
      " [-0.11445606]] \n",
      "epoch:12, d_loss:11.166023254394531, g_loss: 0.8112698793411255, disc_tr: [[ 0.23012394]\n",
      " [ 0.30407742]], disc_gen: [[-0.22553191]\n",
      " [-0.22197637]] \n",
      "epoch:13, d_loss:11.097810745239258, g_loss: 0.9202850461006165, disc_tr: [[ 0.14506726]\n",
      " [ 0.38883352]], disc_gen: [[-0.40863776]\n",
      " [-0.41558444]] \n",
      "epoch:14, d_loss:10.470911026000977, g_loss: 1.6536808013916016, disc_tr: [[ 1.92120481]\n",
      " [ 1.35943508]], disc_gen: [[-1.44104826]\n",
      " [-1.44154918]] \n",
      "epoch:15, d_loss:10.18600845336914, g_loss: 4.912322521209717, disc_tr: [[ 2.68570971]\n",
      " [ 4.55243874]], disc_gen: [[-4.99471903]\n",
      " [-4.81510162]] \n",
      "epoch:16, d_loss:9.93966293334961, g_loss: 13.702790260314941, disc_tr: [[ 12.48876953]\n",
      " [  5.22649956]], disc_gen: [[-13.59872818]\n",
      " [-13.80685043]] \n",
      "epoch:17, d_loss:9.922216415405273, g_loss: 6.955425262451172, disc_tr: [[ 23.68191528]\n",
      " [ 29.8928299 ]], disc_gen: [[-6.53964853]\n",
      " [-7.36912775]] \n",
      "epoch:18, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:19, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:20, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:21, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:22, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:23, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:24, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:25, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:26, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:27, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:28, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:29, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:30, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:31, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:32, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:33, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:34, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:35, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:36, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:37, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:38, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:39, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:40, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:41, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:42, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:43, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:44, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:45, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:46, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:47, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:48, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:49, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:50, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:51, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:52, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:53, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:54, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:55, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:56, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:57, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:58, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:59, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:60, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:61, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:62, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:63, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:64, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:65, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:66, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:67, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:68, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:69, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:70, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:71, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:72, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:73, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:74, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:75, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:76, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:77, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:78, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n",
      "epoch:79, d_loss:nan, g_loss: nan, disc_tr: [[ nan]\n",
      " [ nan]], disc_gen: [[ nan]\n",
      " [ nan]] \n"
     ]
    }
   ],
   "source": [
    "gan = GAN()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
