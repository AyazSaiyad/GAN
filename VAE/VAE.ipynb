{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE implementation by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utility関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_metrics(metrics, epoch=None):\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"dis_loss\"], label=\"discriminative loss\", color=\"b\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(\"metrics\", \"dloss\" + str(epoch) + \".png\"))\n",
    "    plt.close()\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics, label=\"generative loss\", color=\"r\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(\"metrics\", \"g_loss\" + str(epoch) + \".png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noise[[examples, 100]]から生成した画像をplot_dim(例えば4x4)で表示\n",
    "def save_imgs(images, plot_dim=(5,12), size=(12,5), epoch=None):\n",
    "    #examples = plot_dim[0]*plot_dim[1]\n",
    "    examples = 60\n",
    "    #print(epoch)\n",
    "    # 表示\n",
    "    fig = plt.figure(figsize=size)\n",
    "    for i in range(examples):\n",
    "        plt.subplot(5, 12, i+1)\n",
    "        img = images[i, :]\n",
    "        img = img.reshape((96, 96, 3))\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.savefig(os.path.join(\"generated_figures\", str(epoch) + \".png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    def __init__(self):\n",
    "        self.x_dim = 96*96*3\n",
    "#        self.h_dim = config.h_dim\n",
    "        self.z_dim = 100\n",
    "\n",
    "        self.initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01, dtype=tf.float32)\n",
    "\n",
    "        print(\"init\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        print(\"called\")\n",
    "        x_tensored = tf.convert_to_tensor(x)\n",
    "        x_tensored = tf.reshape(x_tensored, [-1, 96*96*3])\n",
    "\n",
    "        # Hindden Layer Encoder\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            weight = tf.get_variable(\"W3\", shape=[96*96*3, 1000], initializer=self.initializer)\n",
    "            bias = tf.get_variable(\"b3\", shape=[1000], initializer=self.initializer)\n",
    "            enc1 = tf.matmul(x_tensored, weight) + bias\n",
    "            enc1 = tf.tanh(enc1)\n",
    "\n",
    "            weight = tf.get_variable(\"W4\", shape=[1000, 500], initializer=self.initializer)\n",
    "            bias = tf.get_variable(\"b4\", shape=[500], initializer=self.initializer)\n",
    "            enc2 = tf.matmul(enc1, weight) + bias\n",
    "            enc2 = tf.tanh(enc2)\n",
    "\n",
    "            weight = tf.get_variable(\"W5\", shape=[500, 250], initializer=self.initializer)\n",
    "            bias = tf.get_variable(\"b5\", shape=[250], initializer=self.initializer)\n",
    "            enc3 = tf.matmul(enc2, weight) + bias\n",
    "#            enc3 = tf.nn.relu(enc3)\n",
    "        \n",
    "        # Mu Encoder\n",
    "        with tf.variable_scope(\"enc_mu\"):\n",
    "            weight = tf.get_variable(\"W\", shape=[250,self.z_dim], initializer=self.initializer)\n",
    "            bias = tf.get_variable(\"b\", shape=[self.z_dim], initializer=self.initializer)\n",
    "            enc_mu = tf.matmul(enc3, weight) + bias\n",
    "            enc_mu = tf.nn.relu(enc_mu)\n",
    "        \n",
    "        # Sigma Encoder\n",
    "        with tf.variable_scope(\"enc_logsg\"):\n",
    "            weight = tf.get_variable(\"W\", shape=[250,self.z_dim], initializer=self.initializer)\n",
    "            bias = tf.get_variable(\"b\", shape=[self.z_dim], initializer=self.initializer)\n",
    "            enc_logsd = tf.matmul(enc3, weight) + bias\n",
    "            enc_logsd = tf.nn.relu(enc_logsd)\n",
    "        \n",
    "        # Sample Epsilon\n",
    "        epsilon = tf.random_normal(tf.shape(enc_mu), name=\"epsilon\")\n",
    "        \n",
    "        # Sample Latent Variable\n",
    "        std_encoder = tf.exp(enc_logsd/2)\n",
    "        \n",
    "        # Compute KL divergence\n",
    "        KLD = -0.5 * tf.reduce_sum(1 + enc_logsd - tf.pow(enc_mu, 2) - tf.exp(enc_logsd), reduction_indices=1)\n",
    "\n",
    "        # Generate z\n",
    "        # z = mu + (sigma * epsilon)\n",
    "        z = enc_mu + tf.multiply(std_encoder, epsilon)\n",
    "\n",
    "        # Hidden Layer decoder\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            weight = tf.get_variable(\"W1\", shape=[self.z_dim,250], initializer=self.initializer)\n",
    "            bias = tf.get_variable(\"b1\", shape=[250], initializer=self.initializer)\n",
    "            dec1 = tf.matmul(z, weight) + bias\n",
    "            dec1 = tf.tanh(dec1)\n",
    "\n",
    "            weight = tf.get_variable(\"W2\", shape=[250,500], initializer=self.initializer)\n",
    "            bias = tf.get_variable(\"b2\", shape=[500], initializer=self.initializer)\n",
    "            dec2 = tf.matmul(dec1, weight) + bias\n",
    "            dec2 = tf.tanh(dec2)\n",
    "\n",
    "            weight = tf.get_variable(\"W3\", shape=[500,1000], initializer=self.initializer)\n",
    "            bias = tf.get_variable(\"b3\", shape=[1000], initializer=self.initializer)\n",
    "            dec3 = tf.matmul(dec2, weight) + bias\n",
    "            dec3 = tf.tanh(dec3)\n",
    "\n",
    "            weight = tf.get_variable(\"W4\", shape=[1000, 96*96*3], initializer=self.initializer)\n",
    "            bias = tf.get_variable(\"b4\", shape=[96*96*3], initializer=self.initializer)\n",
    "            dec4 = tf.matmul(dec3, weight) + bias\n",
    "            dec4 = tf.sigmoid(dec4)\n",
    "\n",
    "        # Compute binary cross entropy(reconstruction loss)\n",
    "        BCE = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=dec4, labels=x_tensored), reduction_indices=1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tf.reduce_mean(KLD + BCE)\n",
    "\n",
    "        return dec4, loss\n",
    "\n",
    "    def train(self, loss):\n",
    "        train_op = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load(\"irasutoya_face_1813x96x96x3_jpg.npy\")\n",
    "X_train = X_train/255\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "\n",
    "metrics_save_epoch = 10\n",
    "img_save_epoch = 2\n",
    "\n",
    "param_save_epoch = 10000\n",
    "losses = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_imgs = tf.placeholder(tf.float32, shape=[batch_size, 96, 96, 3])\n",
    "#z = tf.placeholder(tf.float32, shape=[batch_size, z_dim])\n",
    "\n",
    "AE = VAE()\n",
    "\n",
    "pred, ae_loss = AE(train_imgs)\n",
    "train_op = AE.train(ae_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #print(epoch)\n",
    "\n",
    "        # 訓練データを抜粋\n",
    "        rand_index = np.random.randint(0, X_train.shape[0], size=batch_size)\n",
    "        exImgs = X_train[rand_index, :].astype(np.float32)\n",
    "\n",
    "        loss, _ = sess.run([ae_loss, train_op], feed_dict={train_imgs: exImgs})\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        if epoch % metrics_save_epoch == 0:\n",
    "\n",
    "            save_metrics(losses, epoch)\n",
    "        \n",
    "        if epoch % img_save_epoch == 0:\n",
    "            print(\"epoch:\" + str(epoch) + \", loss:\" + str(loss))\n",
    "            imgs = sess.run(pred, feed_dict={train_imgs: exImgs})\n",
    "            #print(imgs.shape)\n",
    "            imgs = imgs.reshape(imgs.shape[0], 96, 96, 3)\n",
    "            #print(imgs.shape)\n",
    "            save_imgs(imgs, epoch=epoch)\n",
    "            save_imgs(imgs*255, epoch=\"_\"+str(epoch))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
