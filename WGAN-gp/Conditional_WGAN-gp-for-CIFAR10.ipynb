{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cramer GAN implementation by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# utility func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save metrics\n",
    "def save_metrics(metrics, epoch=None):\n",
    "    # make directory if there is not\n",
    "    path = \"metrics\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # save metrics\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"d_loss\"], label=\"discriminative loss\", color=\"b\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"dloss\" + str(epoch) + \".png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"g_loss\"], label=\"generative loss\", color=\"r\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"g_loss\" + str(epoch) + \".png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"g_loss\"], label=\"generative loss\", color=\"r\")\n",
    "    plt.plot(metrics[\"d_loss\"], label=\"discriminative loss\", color=\"b\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"both_loss\" + str(epoch) + \".png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot images\n",
    "def save_imgs(images, plot_dim=(10,6), size=(6,10), epoch=None):\n",
    "    # make directory if there is not\n",
    "    path = \"generated_figures\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    num_examples = plot_dim[0]*plot_dim[1]\n",
    "    num_examples = 60\n",
    "    fig = plt.figure(figsize=size)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        plt.subplot(plot_dim[0], plot_dim[1], i+1)\n",
    "        img = images[i, :]\n",
    "        img = img.reshape((32, 32, 3))\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.savefig(os.path.join(path, str(epoch) + \".png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    #print(file)\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def one_hot_vec(label):\n",
    "    vec = np.zeros(10)\n",
    "    vec[label] = 1\n",
    "    return vec\n",
    "\n",
    "def load_data():\n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    for i in range (5):\n",
    "        d = unpickle(\"cifar-10-batches-py/data_batch_\" + str(i+1))\n",
    "        x_ = d['data']\n",
    "        y_ = d['labels']\n",
    "        x_all.append(x_)\n",
    "        y_all.append(y_)\n",
    "\n",
    "    d = unpickle('cifar-10-batches-py/test_batch')\n",
    "    x_all.append(d['data'])\n",
    "    y_all.append(d['labels'])\n",
    "\n",
    "    x = np.concatenate(x_all) / np.float32(255)\n",
    "    y = np.concatenate(y_all)\n",
    "    x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3))\n",
    "\n",
    "    #pixel_mean = np.mean(x[0:50000],axis=0)\n",
    "    #x -= pixel_mean\n",
    "    y = np.array(list(map(one_hot_vec, y)))\n",
    "    #X_train = x[0:50000,:,:,:]\n",
    "    #Y_train = y[0:50000]\n",
    "    #X_test = x[50000:,:,:,:]\n",
    "    #Y_test = y[50000:]\n",
    "\n",
    "    #return (X_train, Y_train, X_test, Y_test)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.initializer = tf.contrib.layers.xavier_initializer()\n",
    "        self.X_dim = 32*32*3\n",
    "        self.z_dim = 100\n",
    "        self.y_dim = 10\n",
    "\n",
    "        self.depths = [1024, 512, 256, 128, 3]\n",
    "        self.s_size = 6\n",
    "\n",
    "    def __call__(self, z, y, training=False):\n",
    "        with tf.variable_scope('g', reuse=self.reuse):\n",
    "            #print(z.get_shape())\n",
    "            #print(y.get_shape())\n",
    "            inputs = tf.concat([z, y], 1)\n",
    "            inputs = tf.reshape(inputs, [-1, self.z_dim + self.y_dim])\n",
    "            fc1 = inputs\n",
    "#            fc1 = tf.layers.dense(inputs, 1024)\n",
    "#            fc1 = tf.nn.relu(fc1)\n",
    "            fc2 = tf.layers.dense(fc1, 2*2*self.depths[0])\n",
    "            fc2 = tf.reshape(fc2, [-1, 2,2,self.depths[0]])\n",
    "            fc2 = tf.nn.relu(fc2)\n",
    "            conv1 = tf.contrib.layers.conv2d_transpose(fc2, self.depths[1], [4,4],[2,2]) # 4\n",
    "            conv1 = tf.nn.relu(conv1)\n",
    "            conv2 = tf.contrib.layers.conv2d_transpose(conv1, self.depths[2], [4,4],[2,2]) # 8\n",
    "            conv2 = tf.nn.relu(conv2)\n",
    "            conv3 = tf.contrib.layers.conv2d_transpose(conv2, self.depths[3], [4,4],[2,2]) # 16\n",
    "            conv3 = tf.nn.relu(conv3)\n",
    "            conv4 = tf.contrib.layers.conv2d_transpose(conv3, self.depths[4], [4,4],[2,2], activation_fn=tf.sigmoid)\n",
    "            conv4 = tf.reshape(conv4, [-1, 32*32*3])\n",
    "            outputs = conv4\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='g')\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.X_dim = 32*32*3\n",
    "        self.initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        self.depths = [3, 64, 128, 256, 512]\n",
    "        self.s_size = 6\n",
    "\n",
    "    def __call__(self, x, y, training=False, name=''):\n",
    "        def leaky_relu(x, leak=0.2, name='outputs'):\n",
    "            return tf.maximum(x, x * leak, name=name)\n",
    "\n",
    "        with tf.name_scope('d' + name), tf.variable_scope('d', reuse=self.reuse):\n",
    "            x = tf.reshape(x, [-1, 32*32*3])\n",
    "            y = tf.reshape(y, [-1, 10])\n",
    "            inputs = tf.concat([x, y], 1)\n",
    "            inputs = tf.layers.dense(inputs, 32*32*3)\n",
    "            x = tf.reshape(inputs, [-1, 32, 32, 3])\n",
    "            conv1 = tf.layers.conv2d(x, self.depths[1], [4,4], [2,2]) # 16\n",
    "            conv1 = leaky_relu(conv1)\n",
    "            conv2 = tf.layers.conv2d(conv1, self.depths[2], [4,4], [2,2]) # 8\n",
    "            conv2 = leaky_relu(conv2)\n",
    "            conv3 = tf.layers.conv2d(conv2, self.depths[3], [4,4], [2,2]) # 4\n",
    "            conv3 = leaky_relu(conv3)\n",
    "            #conv4 = tf.layers.conv2d(conv3, self.depths[4], [4,4], [2,2]) # 2\n",
    "            #conv4 = leaky_relu(conv4)\n",
    "            conv4 = tf.contrib.layers.flatten(conv3) \n",
    "            fc1 = tf.layers.dense(conv4, 512)\n",
    "            fc1 = leaky_relu(fc1)\n",
    "            fc2 = tf.layers.dense(fc1, 1)\n",
    "            \"\"\"\n",
    "            # convolution x 4\n",
    "            outputs = tf.reshape(inputs, [-1, 96, 96, 3])\n",
    "\n",
    "            with tf.variable_scope('conv1'):\n",
    "                outputs = tf.layers.conv2d(outputs, self.depths[1], [5, 5], strides=(2, 2), padding='SAME')\n",
    "                outputs = leaky_relu(outputs, name='outputs')\n",
    "            with tf.variable_scope('conv2'):\n",
    "                outputs = tf.layers.conv2d(outputs, self.depths[2], [5, 5], strides=(2, 2), padding='SAME')\n",
    "                outputs = leaky_relu(outputs, name='outputs')\n",
    "            with tf.variable_scope('conv3'):\n",
    "                outputs = tf.layers.conv2d(outputs, self.depths[3], [5, 5], strides=(2, 2), padding='SAME')\n",
    "                outputs = leaky_relu(outputs, name='outputs')\n",
    "            with tf.variable_scope('conv4'):\n",
    "                outputs = tf.layers.conv2d(outputs, self.depths[4], [5, 5], strides=(2, 2), padding='SAME')\n",
    "                outputs = leaky_relu(outputs, name='outputs')\n",
    "            with tf.variable_scope('classify'):\n",
    "                outputs = tf.layers.dense(outputs, 256, name='outputs')\n",
    "            \"\"\"\n",
    "        outputs = fc2\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='d')\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f(x) = ||h(x) - h(x_)|| - ||h(x)||\n",
    "class Critic(object):\n",
    "    def __init__(self, h):\n",
    "        self.h = h\n",
    "    def __call__(self, x, x_):\n",
    "        return tf.norm(self.h(x) - self.h(x_), axis=1) - tf.norm(self.h(x), axis=1)\n",
    "\n",
    "# f(x) = ||h(x) - h(x_)||\n",
    "class calc_Norm(object):\n",
    "    def __init__(self, h):\n",
    "        self.h = h\n",
    "    def __call__(self, x, x_):\n",
    "        return tf.norm(self.h(x) -self.h(x_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_concat(x, y):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    y = tf.reshape(y, [batch_size, 1, 1, 10])\n",
    "    return tf.concat([x, y], 1) # bzx32x32x3 + bzx1x1x10\n",
    "\n",
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 256\n",
    "        img_size = 32\n",
    "        num_class = 10\n",
    "        self.z_dim = 100\n",
    "\n",
    "        self.epochs = 1000000\n",
    "        self.epoch_saveMetrics = 500\n",
    "        self.epoch_saveSampleImg = 500\n",
    "        self.epoch_saveParamter = 10000\n",
    "        self.losses = {\"d_loss\":[], \"g_loss\":[]}\n",
    "\n",
    "        self.x, self.y = load_data()\n",
    "\n",
    "        self.X_tr = tf.placeholder(tf.float32, shape=[None, img_size, img_size, 3])\n",
    "        self.Y_tr = tf.placeholder(tf.float32, shape=[None, num_class])\n",
    "        self.Z1 = tf.placeholder(tf.float32, [None, self.z_dim])\n",
    "        \n",
    "        self.g = Generator()\n",
    "        self.d = Discriminator()\n",
    "        self.Xg = self.g(self.Z1, self.Y_tr)\n",
    "\n",
    "    def loss(self, Xr, Yr, z1):\n",
    "        Xg = self.g(z1, Yr)\n",
    "        Xg = tf.reshape(Xg, [self.batch_size, -1]) \n",
    "        Xr = tf.reshape(Xr, [self.batch_size, -1])\n",
    "        epsilon = tf.random_uniform([], 0.0, 1.0) # 怪しい\n",
    "        Xhat = epsilon * Xr + (1-epsilon) * Xg   \n",
    "        dhat = self.d(Xhat, Yr)\n",
    "        ddx = tf.gradients(dhat, Xhat)[0]\n",
    "        #print((ddx.get_shape().as_list()))\n",
    "        ddx = tf.norm(ddx, axis=1)\n",
    "        ddx = tf.reduce_mean(tf.square(ddx - 1.0) * 10)      \n",
    "  \n",
    "        L_d = self.d(Xg, Yr) - self.d(Xr, Yr) + ddx\n",
    "        L_g = -self.d(Xg, Yr)\n",
    "        return L_g, L_d\n",
    "\n",
    "    def train(self):\n",
    "        # Optimizer\n",
    "        learning_rate = 1e-4\n",
    "        beta1 = 0.5\n",
    "        beta2 = 0.9\n",
    "\n",
    "        self.L_g, self.L_d = self.loss(Xr=self.X_tr, Yr=self.Y_tr, z1=self.Z1)\n",
    "\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2)\n",
    "        d_train_op = d_opt.minimize(self.L_d, var_list=self.d.variables)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2)\n",
    "        g_train_op = g_opt.minimize(self.L_g, var_list=self.g.variables)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        #%debug\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #rom tensorflow.python import debug as tf_debug\n",
    "            #ess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "\n",
    "            z_test = np.random.uniform(-1, 1, size=[60, self.z_dim])\n",
    "            y_tmp = np.repeat(np.arange(10), 6)\n",
    "            y_test = np.array(list(map(one_hot_vec, y_tmp)))\n",
    "            print(y_test.shape)\n",
    "            print(y_test)\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "\n",
    "                # X_mb, Y_mbを収集\n",
    "                def extractXimg(path, batch_size):\n",
    "                    imgs = os.listdir(path)\n",
    "                    rand_id = np.random.randint(0, len(imgs), size=batch_size)\n",
    "                    X_mb = np.zeros((1, 32, 32, 3))\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        img = Image.open(path+\"/\"+imgs[rand_id[i]])\n",
    "                        img_np = np.asarray(img)\n",
    "                        X_mb = np.vstack((X_mb, img_np[np.newaxis, :]))\n",
    "                        img.close()\n",
    "                    X_mb = X_mb[1:,:]\n",
    "                    return X_mb\n",
    "                \n",
    "                for _ in range(5):\n",
    "                    # 訓練データを抜粋\n",
    "                    z1 = np.random.uniform(-1, 1, size=[self.batch_size, self.z_dim])\n",
    "\n",
    "                    rand_id = np.random.randint(0, self.x.shape[0], size=self.batch_size)\n",
    "                    X_mb = self.x[rand_id]\n",
    "                    Y_mb = self.y[rand_id]\n",
    "\n",
    "                    # X_mb = X_mb/255\n",
    "                    X_mb = X_mb.astype(np.float32)\n",
    "\n",
    "                    # Y_mb = Y_mb/255\n",
    "                    Y_mb = Y_mb.astype(np.float32)\n",
    "                    \n",
    "                    _, d_loss_value = sess.run([d_train_op, self.L_d], feed_dict={self.X_tr: X_mb, self.Y_tr: Y_mb, self.Z1:z1})\n",
    "\n",
    "                # train G\n",
    "                _, g_loss_value = sess.run([g_train_op, self.L_g], feed_dict={self.X_tr: X_mb, self.Y_tr: Y_mb, self.Z1: z1})\n",
    "\n",
    "                # generate Sample Imgs\n",
    "                #sampleImgsOfX2Y, sampleImgsOfY2X = sess.run([self.X2Y, self.Y2X], feed_dict={self.X_tr: X_mb, self.Y_tr: Y_mb})\n",
    "\n",
    "                # 結果をappend\n",
    "                self.losses[\"d_loss\"].append(np.sum(d_loss_value))\n",
    "                self.losses[\"g_loss\"].append(np.sum(g_loss_value))\n",
    "                \n",
    "                if epoch % 100 == 0:\n",
    "                    print(\"epoch:\" + str(epoch))\n",
    "\n",
    "                # lossの可視化\n",
    "                if epoch % self.epoch_saveMetrics == 1:\n",
    "                    save_metrics(self.losses, epoch)\n",
    "\n",
    "                # 画像の変換テスト\n",
    "                if epoch % self.epoch_saveSampleImg == 0:\n",
    "                    img = sess.run(self.Xg, feed_dict={self.Z1: z_test, self.Y_tr: y_test})\n",
    "                    save_imgs(img, epoch=str(epoch))\n",
    "                    # imgs_Y = np.vstack((sampleImgsOfX2Y, Y_mb))\n",
    "                    # imgs_X = np.vstack((sampleImgsOfY2X, X_mb))\n",
    "                    \n",
    "                    # save_imgs(imgs_Y, epoch=str(epoch)+\"X2Y\")\n",
    "                    #save_imgs(imgs_X, epoch=str(epoch)+\"Y2X\")\n",
    "                    \n",
    "                    #save_imgs(sampleImgsOfX2Y, epoch=str(epoch)+\"X2Y\")\n",
    "                    #save_imgs(sampleImgsOfY2X, epoch=str(epoch)+\"Y2X\")\n",
    "\n",
    "                # parameterのsave\n",
    "                if epoch % self.epoch_saveParamter == 1:\n",
    "                    path = \"model\"\n",
    "                    if not os.path.isdir(path):\n",
    "                        os.makedirs(path)\n",
    "\n",
    "                    saver.save(sess, \"./model/dcgan_model\" + str(epoch) + \".ckpt\")\n",
    "       \n",
    "\n",
    "    def sample_images(self, row=5, col=12, inputs=None, epoch=None):\n",
    "        images = self.g(inputs, training=True)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan = GAN()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load(\"irasutoya_face_1813x96x96x3_jpg.npy\")\n",
    "X_train = X_train/255\n",
    "X_dim = 96*96*3\n",
    "z_dim = 100\n",
    "batch_size = 32\n",
    "epochs = 500000\n",
    "display_epoch = 100\n",
    "param_save_epoch = 10000\n",
    "loss = {\"dis_loss\":[], \"gen_loss\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_noise = tf.placeholder(tf.float32, [None, z_dim])\n",
    "noise_check = np.random.uniform(-1, 1, size=[60, z_dim]).astype(np.float32)\n",
    "\n",
    "Xr = tf.placeholder(tf.float32, [None, X_dim])\n",
    "Z1 = tf.placeholder(tf.float32, [None, z_dim])\n",
    "#Z2 = tf.placeholder(tf.float32, [None, z_dim])\n",
    "\n",
    "gan = GAN()\n",
    "\n",
    "losses = gan.loss(Xr=Xr, z1=Z1)\n",
    "d_train_op = gan.d_train(losses)\n",
    "g_train_op = gan.g_train(losses)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#%debug\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % display_epoch == 0:\n",
    "            imgs_gen = gan.sample_images(inputs=noise_check).eval()\n",
    "            print(\"saving images\")\n",
    "            save_imgs(imgs_gen, epoch=epoch)\n",
    "\n",
    "        for _ in range(5):\n",
    "            # 訓練データを抜粋\n",
    "            rand_index = np.random.randint(0, X_train.shape[0], size=batch_size)\n",
    "            X_mb = X_train[rand_index, :].astype(np.float32)\n",
    "            X_mb = X_mb.reshape([-1, X_dim])\n",
    "\n",
    "            z1 = np.random.uniform(-1, 1, size=[batch_size, z_dim])\n",
    "            _, d_loss_value = sess.run([d_train_op, losses[gan.d]], feed_dict={Xr: X_mb, Z1:z1})\n",
    "\n",
    "\n",
    "        # 訓練データを抜粋\n",
    "        rand_index = np.random.randint(0, X_train.shape[0], size=batch_size)\n",
    "        X_mb = X_train[rand_index, :].astype(np.float32)\n",
    "        X_mb = X_mb.reshape([-1, X_dim])\n",
    "        z1 = np.random.uniform(-1, 1, size=[batch_size, z_dim])\n",
    "        _, g_loss_value, = sess.run([g_train_op, losses[gan.g]], feed_dict={Xr: X_mb, Z1:z1})\n",
    "\n",
    "        # 結果をappend\n",
    "        loss[\"dis_loss\"].append(d_loss_value)\n",
    "        loss[\"gen_loss\"].append(g_loss_value)\n",
    "        print(\"epoch:\" + str(epoch))\n",
    "        # グラフの描画（余裕があったら）\n",
    "        if epoch % display_epoch == 0:\n",
    "            save_metrics(loss, epoch)\n",
    "\n",
    "        if epoch % param_save_epoch == 0:\n",
    "            path = \"model\"\n",
    "            if not os.path.isdir(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "            saver.save(sess, \"./model/dcgan_model\" + str(epoch) + \".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(10)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "\n",
    "b = a - 1\n",
    "print(b)\n",
    "print(b.shape)\n",
    "print(np.abs(b))\n",
    "#np.abs\n",
    "#tf.pow((tf.abs(delta)-1), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [[[0],[1],[2]],\n",
    "     [[0],[1],[2]],\n",
    "     [[0],[1],[2]],\n",
    "     [[0],[1],[2]]]\n",
    "\n",
    "b = tf.reduce_sum(a, axis=0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    c = sess.run(b)\n",
    "    \n",
    "print(np.array(a).shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x = np.ones([32, 32, 32, 3])\n",
    "y = np.zeros([32, 1, 1, 10])\n",
    "\n",
    "tf.concat([x, y], 3)\n",
    "\n",
    "print(x.shape)\n",
    "#print(y)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
