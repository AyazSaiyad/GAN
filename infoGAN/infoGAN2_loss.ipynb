{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "(1, 10)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "a,b  = mnist.train.next_batch(1)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# utility func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save metrics\n",
    "def save_metrics(metrics, epoch=None):\n",
    "    # make directory if there is not\n",
    "    path = \"metrics_gpu1\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # save metrics\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"d_loss\"], label=\"discriminative loss\", color=\"b\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"dloss\" + str(epoch) + \".png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"g_loss\"], label=\"generative loss\", color=\"r\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"g_loss\" + str(epoch) + \".png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"g_loss\"], label=\"generative loss\", color=\"r\")\n",
    "    plt.plot(metrics[\"d_loss\"], label=\"discriminative loss\", color=\"b\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"both_loss\" + str(epoch) + \".png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot images\n",
    "def save_imgs(images, plot_dim=(10,10), size=(10,10), name=None):\n",
    "    # make directory if there is not\n",
    "    path = \"generated_figures_gpu1\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    num_examples = plot_dim[0]*plot_dim[1]\n",
    "    num_examples = 100\n",
    "    fig = plt.figure(figsize=size)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        plt.subplot(plot_dim[0], plot_dim[1], i+1)\n",
    "        img = images[i, :]\n",
    "        img = img.reshape((28, 28))\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.savefig(os.path.join(path, str(name) + \".png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    #print(file)\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def one_hot_vec(label):\n",
    "    vec = np.zeros(10)\n",
    "    vec[label] = 1\n",
    "    return vec\n",
    "\n",
    "def load_data():\n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    for i in range (5):\n",
    "        d = unpickle(\"cifar-10-batches-py/data_batch_\" + str(i+1))\n",
    "        x_ = d['data']\n",
    "        y_ = d['labels']\n",
    "        x_all.append(x_)\n",
    "        y_all.append(y_)\n",
    "\n",
    "    d = unpickle('cifar-10-batches-py/test_batch')\n",
    "    x_all.append(d['data'])\n",
    "    y_all.append(d['labels'])\n",
    "\n",
    "    x = -0.5 + (np.concatenate(x_all) / np.float32(255))\n",
    "    y = np.concatenate(y_all)\n",
    "    x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3))\n",
    "\n",
    "    #pixel_mean = np.mean(x[0:50000],axis=0)\n",
    "    #x -= pixel_mean\n",
    "    y = np.array(list(map(one_hot_vec, y)))\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "    #X_test = x[50000:,:,:,:]\n",
    "    #Y_test = y[50000:]\n",
    "\n",
    "    #return (X_train, Y_train, X_test, Y_test)\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convolution/pool stride\n",
    "_CONV_KERNEL_STRIDES_ = [1, 2, 2, 1]\n",
    "_DECONV_KERNEL_STRIDES_ = [1, 2, 2, 1]\n",
    "_REGULAR_FACTOR_ = 1.0e-4\n",
    "\n",
    "def conv2d_layer(input_layer, output_dim, kernel_size = 3, stddev = 0.02, name = 'conv2d'):\n",
    "    with tf.variable_scope(name):\n",
    "        init_weight = tf.truncated_normal_initializer(mean = 0.0, stddev = stddev, dtype = tf.float32)\n",
    "        filter_size = [kernel_size, kernel_size, input_layer.get_shape()[-1], output_dim]\n",
    "        weight = tf.get_variable(\n",
    "            name = name + 'weight',\n",
    "            shape = filter_size,\n",
    "            initializer = init_weight,\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(_REGULAR_FACTOR_))\n",
    "        bias = tf.get_variable(\n",
    "            name = name + 'bias',\n",
    "            shape = [output_dim],\n",
    "            initializer = tf.constant_initializer(0.0))\n",
    "        conv = tf.nn.conv2d(input_layer, weight, _CONV_KERNEL_STRIDES_, padding = 'SAME')\n",
    "        conv = tf.nn.bias_add(conv, bias)\n",
    "        return conv\n",
    "\n",
    "def deconv2d_layer(input_layer, output_shape, kernel_size = 2, stddev = 0.02, name = 'deconv'):\n",
    "    with tf.variable_scope(name):\n",
    "        init_weight = tf.truncated_normal_initializer(mean = 0.0, stddev = stddev, dtype = tf.float32)\n",
    "        filter_size = [kernel_size, kernel_size, output_shape[-1], input_layer.get_shape()[-1]]\n",
    "        weight = tf.get_variable(\n",
    "            name = name + 'weight',\n",
    "            shape = filter_size,\n",
    "            initializer = init_weight,\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(_REGULAR_FACTOR_))\n",
    "        bias = tf.get_variable(\n",
    "            name = name + 'bias',\n",
    "            shape = [output_shape[-1]],\n",
    "            initializer = tf.constant_initializer(0.0))\n",
    "        deconv = tf.nn.conv2d_transpose(input_layer, weight, output_shape, strides = _DECONV_KERNEL_STRIDES_, padding = 'SAME')\n",
    "        deconv = tf.nn.bias_add(deconv, bias)\n",
    "        return deconv\n",
    "\n",
    "def lrelu(input_layer, leak = 0.2, name = 'lrelu'):\n",
    "    with tf.variable_scope(name):\n",
    "        alpha1 = 0.5 * (1 + leak)\n",
    "        alpha2 = 0.5 * (1 - leak)\n",
    "        return alpha1 * input_layer + alpha2 * abs(input_layer)\n",
    "\n",
    "def full_connection_layer(input_layer, output_dim, stddev = 0.02, name = 'fc'):\n",
    "    # calculate input_layer dimension and reshape to batch * dimension\n",
    "    input_dimension = 1\n",
    "    for dim in input_layer.get_shape().as_list()[1:]:\n",
    "        input_dimension *= dim\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        init_weight = tf.truncated_normal_initializer(mean = 0.0, stddev = stddev, dtype = tf.float32)\n",
    "        filter_size = [input_dimension, output_dim]\n",
    "        weight = tf.get_variable(\n",
    "            name = name + 'weight',\n",
    "            shape = filter_size,\n",
    "            initializer = init_weight,\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(_REGULAR_FACTOR_))\n",
    "        bias = tf.get_variable(\n",
    "            name = name + 'bias',\n",
    "            shape = [output_dim],\n",
    "            initializer = tf.constant_initializer(0.0))\n",
    "        input_layer_reshape = tf.reshape(input_layer, [-1, input_dimension])\n",
    "        fc = tf.matmul(input_layer_reshape, weight)\n",
    "        tc = tf.nn.bias_add(fc, bias)\n",
    "        return fc\n",
    "\n",
    "class BatchNormalization:\n",
    "    def __init__(self, epsilon=1e-5, momentum = 0.9, name=\"batch_norm\"):\n",
    "        with tf.variable_scope(name):\n",
    "            self.epsilon  = epsilon\n",
    "            self.momentum = momentum\n",
    "            self.name = name\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        return tf.contrib.layers.batch_norm(x,\n",
    "                decay=self.momentum, \n",
    "                updates_collections=None,\n",
    "                epsilon=self.epsilon,\n",
    "                scale=True,\n",
    "                is_training=train,\n",
    "                scope=self.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.reuse = False\n",
    "        self.initializer = tf.contrib.layers.xavier_initializer()\n",
    "        self.X_dim = 28*28*1 \n",
    "        self.z_dim = 62\n",
    "\n",
    "        self.g_bn0 = BatchNormalization(name = 'g_bn0')\n",
    "        self.g_bn1 = BatchNormalization(name = 'g_bn1')\n",
    "        self.g_bn2 = BatchNormalization(name = 'g_bn2')\n",
    "\n",
    "    def __call__(self, z, training=False):\n",
    "        with tf.variable_scope('g', reuse=self.reuse):\n",
    "            fc0 = full_connection_layer(z, 1024, name=\"fc0\")\n",
    "            fc0 = self.g_bn0(fc0)\n",
    "            fc0 = tf.nn.relu(fc0)\n",
    "\n",
    "            fc1 = full_connection_layer(fc0, 7*7*128, name=\"fc1\")\n",
    "            fc1 = tf.reshape(fc1, [-1, 7, 7, 128])\n",
    "            fc1 = self.g_bn1(fc1)\n",
    "            fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "            batch_size = tf.shape(fc1)[0]\n",
    "            #batch_size = bs\n",
    "            \n",
    "            deconv0 = deconv2d_layer(fc1, [batch_size, 14, 14, 64], name=\"deconv0\")\n",
    "            deconv0 = self.g_bn2(deconv0)\n",
    "            deconv0 = tf.nn.relu(deconv0)\n",
    "\n",
    "            deconv1 = deconv2d_layer(deconv0, [batch_size, 28, 28, 1], name=\"deconv1\")\n",
    "            output = tf.nn.tanh(deconv1)\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='g')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, cat_size, con_size):\n",
    "        self.reuse = False\n",
    "\n",
    "        self.cat_size = cat_size\n",
    "        self.con_size = con_size        \n",
    "\n",
    "        self.d_bn0 = BatchNormalization(name=\"d_bn0\")\n",
    "        \n",
    "    def __call__(self, x,training=False, name=''):\n",
    "        def leaky_relu(x, leak=0.2, name='outputs'):\n",
    "            return tf.maximum(x, x * leak, name=name)\n",
    "\n",
    "        with tf.name_scope('d' + name), tf.variable_scope('d', reuse=self.reuse):\n",
    "            x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "            conv1 = conv2d_layer(x, 64, name=\"d_conv0\")\n",
    "            conv1 = lrelu(conv1)\n",
    "            \n",
    "            conv2 = conv2d_layer(conv1, 128, name=\"d_conv1\")\n",
    "            conv2 = self.d_bn0(conv2)\n",
    "            conv2 = lrelu(conv2)\n",
    "\n",
    "            fc0 = full_connection_layer(conv2, 1024, name=\"fc0\")\n",
    "            fc0 = tf.nn.relu(fc0)\n",
    "            \n",
    "            fc1 = full_connection_layer(fc0, 128, name=\"fc1\")\n",
    "            fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "            disc = full_connection_layer(fc1, 1, name = 'disc')\n",
    "            cat = full_connection_layer(fc1, self.cat_size, name = 'cat')\n",
    "            con = full_connection_layer(fc1, self.con_size, name = 'con')\n",
    "            print('discriminator ouput dis:', disc.get_shape())\n",
    "            print('discriminator ouput cat:', cat.get_shape())\n",
    "            print('discriminator ouput cont:', con.get_shape())\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='d')\n",
    "        return disc, cat, con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_vec(label):\n",
    "    vec = np.zeros(10)\n",
    "    vec[label] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.img_size = 28\n",
    "        self.cat_size = 10\n",
    "        self.con_size = 2\n",
    "        self.rand_size = 50\n",
    "\n",
    "        self.epochs = 100000\n",
    "        self.epoch_saveMetrics = 3000\n",
    "        self.epoch_saveSampleImg = 3000\n",
    "        self.epoch_saveParamter = 10000\n",
    "        self.losses = {\"d_loss\":[], \"g_loss\":[]}\n",
    "\n",
    "        self.X_tr = tf.placeholder(tf.float32, shape=[None, self.img_size, self.img_size, 1])\n",
    "        self.cat_label = tf.placeholder(tf.int32, [None])\n",
    "        self.cat = tf.placeholder(tf.float32, [None, self.cat_size])\n",
    "        self.con = tf.placeholder(tf.float32, [None, self.con_size])\n",
    "        self.Z1 = tf.placeholder(tf.float32, [None, self.rand_size+self.cat_size+self.con_size])\n",
    "        \n",
    "        self.g = Generator()\n",
    "        self.d = Discriminator(self.cat_size, self.con_size)\n",
    "        self.Xg = self.g(self.Z1)\n",
    "\n",
    "    def loss(self):\n",
    "        disc_tr, cat_tr, con_tr = self.d(self.X_tr)\n",
    "        disc_gen, cat_gen, con_gen = self.d(self.Xg)\n",
    "        \n",
    "        loss_d_tr = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_tr, labels=tf.ones_like(disc_tr)))\n",
    "        loss_d_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen, labels=tf.zeros_like(disc_gen)))\n",
    "        loss_d = (loss_d_tr + loss_d_gen)\n",
    "        \n",
    "        loss_g = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen, labels=tf.ones_like(disc_gen)))\n",
    "        \n",
    "\n",
    "        print(cat_gen.shape)\n",
    "        print(self.cat.get_shape())\n",
    "        # categorical factorloss\n",
    "        loss_cat = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cat_gen, labels=self.cat_label))\n",
    "        \n",
    "        # continuous factor loss\n",
    "        loss_con = tf.reduce_mean(tf.square(con_gen - self.con))\n",
    "\n",
    "        d_cost = loss_d + loss_cat + loss_con\n",
    "        g_cost = loss_g + loss_cat + loss_con\n",
    "    \n",
    "        return g_cost, d_cost\n",
    "\n",
    "    def train(self):\n",
    "        # Optimizer\n",
    "        d_lr = 2e-4\n",
    "        g_lr = 2e-4\n",
    "\n",
    "        self.L_g, self.L_d = self.loss()\n",
    "\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate=d_lr)\n",
    "        d_train_op = d_opt.minimize(self.L_d, var_list=self.d.variables)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=g_lr)\n",
    "        g_train_op = g_opt.minimize(self.L_g, var_list=self.g.variables)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        #%debug\n",
    "\n",
    "        \n",
    "        config = tf.ConfigProto(\n",
    "            gpu_options=tf.GPUOptions(\n",
    "                visible_device_list=\"0\"\n",
    "            )\n",
    "        )\n",
    "                \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            epoch_pre = 0\n",
    "            path = \"model_infoGAN\"\n",
    "            #saver.restore(sess, path+\"/dcgan_model\" + str(epoch_pre) + \".ckpt\")\n",
    "\n",
    "            # visualizing categorical test\n",
    "            bs = 60\n",
    "            cat_test_z_rand = np.random.normal(0, 1, size=[bs, self.rand_size])\n",
    "            cat_test_z_cat_label = np.repeat(np.arange(10), bs/10)\n",
    "            cat_test_z_cat = np.array(list(map(one_hot_vec, cat_test_z_cat_label)))\n",
    "            cat_test_z_con = np.random.normal(0, 1, size=[bs, self.con_size])\n",
    "            cat_test_z = np.concatenate((cat_test_z_rand,cat_test_z_cat,cat_test_z_con), axis=1)\n",
    "\n",
    "            # visualizing continuous factor 1\n",
    "            bs = 100\n",
    "            con1_test_z_rand = np.random.normal(0, 1, size=[bs,self.rand_size])\n",
    "            con1_test_z_cat_label = np.repeat(np.arange(10), bs/10)\n",
    "            con1_test_z_cat = np.array(list(map(one_hot_vec, con1_test_z_cat_label)))\n",
    "            con1_test_z_con1 = np.tile(np.linspace(-1.0, 1.0, num=10), bs//10)[:, np.newaxis]\n",
    "            con1_test_z_con2 = np.zeros([bs])[:, np.newaxis]\n",
    "            con1_test_z = np.concatenate((con1_test_z_rand, con1_test_z_cat, con1_test_z_con1, con1_test_z_con2), axis=1)       \n",
    "\n",
    "            # visualizing continuous factor 2\n",
    "            bs = 100\n",
    "            con2_test_z_rand = np.random.normal(0, 1, size=[bs,self.rand_size])\n",
    "            con2_test_z_cat_label = np.repeat(np.arange(10), bs/10)\n",
    "            con2_test_z_cat = np.array(list(map(one_hot_vec, con2_test_z_cat_label)))\n",
    "            con2_test_z_con2 = np.tile(np.linspace(-1.0, 1.0, num=10), bs//10)[:, np.newaxis]\n",
    "            con2_test_z_con1 = np.zeros([bs])[:, np.newaxis]\n",
    "            con2_test_z = np.concatenate((con2_test_z_rand, con2_test_z_cat, con2_test_z_con1, con2_test_z_con2), axis=1)   \n",
    "\n",
    "            for epoch in range(self.epochs):              \n",
    "                for _ in range(1):\n",
    "                    # 訓練データを抜粋\n",
    "                    X_mb, Y_mb = mnist.train.next_batch(self.batch_size)\n",
    "                    X_mb = np.reshape(X_mb, [-1, 28, 28, 1])\n",
    "                    #X_mb = (X_mb-0.5)*2.0\n",
    "\n",
    "                    z_rand = np.random.normal(-1, 1, size=[self.batch_size, self.rand_size])\n",
    "                    z_cat_label = np.random.randint(0, 10, [self.batch_size])\n",
    "                    #z_cat_label = np.argmax(Y_mb, axis=1)\n",
    "                    z_cat = np.array(list(map(one_hot_vec, z_cat_label)))\n",
    "\n",
    "                    z_con = np.random.normal(0, 1, size=[self.batch_size, self.con_size])\n",
    "                    z = np.concatenate((z_rand,z_cat,z_con), axis=1)\n",
    "                    #from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "                    _, d_loss_value = sess.run([d_train_op, self.L_d], feed_dict={\n",
    "                        self.X_tr: X_mb,\n",
    "                        self.Z1:z,\n",
    "                        self.cat_label:z_cat_label,\n",
    "                        #self.cat: z_cat,\n",
    "                        self.con: z_con,\n",
    "                    })\n",
    "\n",
    "                # train G\n",
    "                _, g_loss_value = sess.run([g_train_op, self.L_g], feed_dict={\n",
    "                        self.X_tr: X_mb,\n",
    "                        self.Z1:z,\n",
    "                        self.cat_label:z_cat_label,\n",
    "                        #self.cat: z_cat,\n",
    "                        self.con: z_con,\n",
    "                })\n",
    "\n",
    "                # generate Sample Imgs\n",
    "                #sampleImgsOfX2Y, sampleImgsOfY2X = sess.run([self.X2Y, self.Y2X], feed_dict={self.X_tr: X_mb, self.Y_tr: Y_mb})\n",
    "\n",
    "                # 結果をappend\n",
    "                self.losses[\"d_loss\"].append(np.sum(d_loss_value))\n",
    "                self.losses[\"g_loss\"].append(np.sum(g_loss_value))\n",
    "                \n",
    "                if epoch % 100 == 0:\n",
    "                    print(\"epoch:\" + str(epoch+epoch_pre))\n",
    "\n",
    "                # lossの可視化\n",
    "                if epoch % self.epoch_saveMetrics == 1:\n",
    "                    save_metrics(self.losses, epoch)\n",
    "\n",
    "                # 画像の変換テスト\n",
    "                if epoch % self.epoch_saveSampleImg == 0:\n",
    "                    con1_img = sess.run(self.Xg, feed_dict={self.Z1: con1_test_z})\n",
    "                    save_imgs(con1_img, name=str(epoch)+\"_con1\")\n",
    "\n",
    "                    con2_img = sess.run(self.Xg, feed_dict={self.Z1:con2_test_z})\n",
    "                    save_imgs(con2_img, name=str(epoch)+\"_con2\")\n",
    "                # parameterのsave\n",
    "                if epoch % self.epoch_saveParamter == 1:\n",
    "                    path = \"model_gpu1\"\n",
    "                    if not os.path.isdir(path):\n",
    "                        os.makedirs(path)\n",
    "\n",
    "                    saver.save(sess, path+\"/dcgan_model\" + str(epoch+epoch_pre) + \".ckpt\")\n",
    "       \n",
    "\n",
    "    def sample_images(self, row=5, col=12, inputs=None, epoch=None):\n",
    "        images = self.g(inputs, training=True)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator ouput dis: (?, 1)\n",
      "discriminator ouput cat: (?, 10)\n",
      "discriminator ouput cont: (?, 2)\n",
      "discriminator ouput dis: (?, 1)\n",
      "discriminator ouput cat: (?, 10)\n",
      "discriminator ouput cont: (?, 2)\n",
      "(?, 10)\n",
      "(?, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujitoko/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:67: DeprecationWarning: PyUnicode_AsEncodedObject() is deprecated; use PyUnicode_AsEncodedString() to encode from str to bytes or PyCodec_Encode() for generic encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "epoch:100\n",
      "epoch:200\n",
      "epoch:300\n",
      "epoch:400\n",
      "epoch:500\n",
      "epoch:600\n",
      "epoch:700\n",
      "epoch:800\n",
      "epoch:900\n",
      "epoch:1000\n",
      "epoch:1100\n",
      "epoch:1200\n",
      "epoch:1300\n",
      "epoch:1400\n",
      "epoch:1500\n",
      "epoch:1600\n",
      "epoch:1700\n",
      "epoch:1800\n",
      "epoch:1900\n",
      "epoch:2000\n",
      "epoch:2100\n",
      "epoch:2200\n",
      "epoch:2300\n",
      "epoch:2400\n",
      "epoch:2500\n",
      "epoch:2600\n",
      "epoch:2700\n",
      "epoch:2800\n",
      "epoch:2900\n",
      "epoch:3000\n",
      "epoch:3100\n",
      "epoch:3200\n",
      "epoch:3300\n",
      "epoch:3400\n",
      "epoch:3500\n",
      "epoch:3600\n",
      "epoch:3700\n",
      "epoch:3800\n",
      "epoch:3900\n",
      "epoch:4000\n",
      "epoch:4100\n",
      "epoch:4200\n",
      "epoch:4300\n",
      "epoch:4400\n",
      "epoch:4500\n",
      "epoch:4600\n",
      "epoch:4700\n",
      "epoch:4800\n",
      "epoch:4900\n",
      "epoch:5000\n",
      "epoch:5100\n",
      "epoch:5200\n",
      "epoch:5300\n",
      "epoch:5400\n",
      "epoch:5500\n",
      "epoch:5600\n",
      "epoch:5700\n",
      "epoch:5800\n",
      "epoch:5900\n",
      "epoch:6000\n",
      "epoch:6100\n",
      "epoch:6200\n",
      "epoch:6300\n",
      "epoch:6400\n",
      "epoch:6500\n",
      "epoch:6600\n",
      "epoch:6700\n",
      "epoch:6800\n",
      "epoch:6900\n",
      "epoch:7000\n",
      "epoch:7100\n",
      "epoch:7200\n",
      "epoch:7300\n",
      "epoch:7400\n",
      "epoch:7500\n",
      "epoch:7600\n",
      "epoch:7700\n",
      "epoch:7800\n",
      "epoch:7900\n",
      "epoch:8000\n",
      "epoch:8100\n",
      "epoch:8200\n",
      "epoch:8300\n",
      "epoch:8400\n",
      "epoch:8500\n",
      "epoch:8600\n",
      "epoch:8700\n",
      "epoch:8800\n",
      "epoch:8900\n",
      "epoch:9000\n",
      "epoch:9100\n",
      "epoch:9200\n",
      "epoch:9300\n",
      "epoch:9400\n",
      "epoch:9500\n",
      "epoch:9600\n",
      "epoch:9700\n",
      "epoch:9800\n",
      "epoch:9900\n",
      "epoch:10000\n",
      "epoch:10100\n",
      "epoch:10200\n",
      "epoch:10300\n",
      "epoch:10400\n",
      "epoch:10500\n",
      "epoch:10600\n",
      "epoch:10700\n",
      "epoch:10800\n",
      "epoch:10900\n",
      "epoch:11000\n",
      "epoch:11100\n",
      "epoch:11200\n",
      "epoch:11300\n",
      "epoch:11400\n",
      "epoch:11500\n",
      "epoch:11600\n",
      "epoch:11700\n",
      "epoch:11800\n",
      "epoch:11900\n",
      "epoch:12000\n",
      "epoch:12100\n",
      "epoch:12200\n",
      "epoch:12300\n",
      "epoch:12400\n",
      "epoch:12500\n",
      "epoch:12600\n",
      "epoch:12700\n",
      "epoch:12800\n",
      "epoch:12900\n",
      "epoch:13000\n",
      "epoch:13100\n",
      "epoch:13200\n",
      "epoch:13300\n",
      "epoch:13400\n",
      "epoch:13500\n",
      "epoch:13600\n",
      "epoch:13700\n",
      "epoch:13800\n",
      "epoch:13900\n",
      "epoch:14000\n",
      "epoch:14100\n",
      "epoch:14200\n",
      "epoch:14300\n",
      "epoch:14400\n",
      "epoch:14500\n",
      "epoch:14600\n",
      "epoch:14700\n",
      "epoch:14800\n",
      "epoch:14900\n",
      "epoch:15000\n",
      "epoch:15100\n",
      "epoch:15200\n",
      "epoch:15300\n",
      "epoch:15400\n",
      "epoch:15500\n",
      "epoch:15600\n",
      "epoch:15700\n",
      "epoch:15800\n",
      "epoch:15900\n",
      "epoch:16000\n",
      "epoch:16100\n",
      "epoch:16200\n",
      "epoch:16300\n",
      "epoch:16400\n",
      "epoch:16500\n",
      "epoch:16600\n",
      "epoch:16700\n",
      "epoch:16800\n",
      "epoch:16900\n",
      "epoch:17000\n",
      "epoch:17100\n",
      "epoch:17200\n",
      "epoch:17300\n",
      "epoch:17400\n",
      "epoch:17500\n",
      "epoch:17600\n",
      "epoch:17700\n",
      "epoch:17800\n",
      "epoch:17900\n",
      "epoch:18000\n",
      "epoch:18100\n",
      "epoch:18200\n",
      "epoch:18300\n",
      "epoch:18400\n",
      "epoch:18500\n",
      "epoch:18600\n",
      "epoch:18700\n",
      "epoch:18800\n",
      "epoch:18900\n",
      "epoch:19000\n",
      "epoch:19100\n",
      "epoch:19200\n",
      "epoch:19300\n",
      "epoch:19400\n",
      "epoch:19500\n",
      "epoch:19600\n",
      "epoch:19700\n",
      "epoch:19800\n",
      "epoch:19900\n",
      "epoch:20000\n",
      "epoch:20100\n",
      "epoch:20200\n",
      "epoch:20300\n",
      "epoch:20400\n",
      "epoch:20500\n",
      "epoch:20600\n",
      "epoch:20700\n",
      "epoch:20800\n",
      "epoch:20900\n",
      "epoch:21000\n",
      "epoch:21100\n",
      "epoch:21200\n",
      "epoch:21300\n",
      "epoch:21400\n",
      "epoch:21500\n",
      "epoch:21600\n",
      "epoch:21700\n",
      "epoch:21800\n",
      "epoch:21900\n",
      "epoch:22000\n",
      "epoch:22100\n",
      "epoch:22200\n",
      "epoch:22300\n",
      "epoch:22400\n",
      "epoch:22500\n",
      "epoch:22600\n",
      "epoch:22700\n",
      "epoch:22800\n",
      "epoch:22900\n",
      "epoch:23000\n",
      "epoch:23100\n",
      "epoch:23200\n",
      "epoch:23300\n",
      "epoch:23400\n",
      "epoch:23500\n",
      "epoch:23600\n",
      "epoch:23700\n",
      "epoch:23800\n",
      "epoch:23900\n",
      "epoch:24000\n",
      "epoch:24100\n",
      "epoch:24200\n",
      "epoch:24300\n",
      "epoch:24400\n",
      "epoch:24500\n",
      "epoch:24600\n",
      "epoch:24700\n",
      "epoch:24800\n",
      "epoch:24900\n",
      "epoch:25000\n",
      "epoch:25100\n",
      "epoch:25200\n",
      "epoch:25300\n",
      "epoch:25400\n",
      "epoch:25500\n",
      "epoch:25600\n",
      "epoch:25700\n",
      "epoch:25800\n",
      "epoch:25900\n",
      "epoch:26000\n",
      "epoch:26100\n",
      "epoch:26200\n",
      "epoch:26300\n",
      "epoch:26400\n",
      "epoch:26500\n",
      "epoch:26600\n",
      "epoch:26700\n",
      "epoch:26800\n",
      "epoch:26900\n",
      "epoch:27000\n",
      "epoch:27100\n",
      "epoch:27200\n",
      "epoch:27300\n",
      "epoch:27400\n",
      "epoch:27500\n",
      "epoch:27600\n",
      "epoch:27700\n",
      "epoch:27800\n",
      "epoch:27900\n",
      "epoch:28000\n",
      "epoch:28100\n",
      "epoch:28200\n",
      "epoch:28300\n",
      "epoch:28400\n",
      "epoch:28500\n",
      "epoch:28600\n",
      "epoch:28700\n",
      "epoch:28800\n",
      "epoch:28900\n",
      "epoch:29000\n",
      "epoch:29100\n",
      "epoch:29200\n",
      "epoch:29300\n",
      "epoch:29400\n",
      "epoch:29500\n",
      "epoch:29600\n",
      "epoch:29700\n",
      "epoch:29800\n",
      "epoch:29900\n",
      "epoch:30000\n",
      "epoch:30100\n",
      "epoch:30200\n",
      "epoch:30300\n",
      "epoch:30400\n",
      "epoch:30500\n",
      "epoch:30600\n",
      "epoch:30700\n",
      "epoch:30800\n",
      "epoch:30900\n",
      "epoch:31000\n",
      "epoch:31100\n",
      "epoch:31200\n",
      "epoch:31300\n",
      "epoch:31400\n",
      "epoch:31500\n",
      "epoch:31600\n",
      "epoch:31700\n",
      "epoch:31800\n",
      "epoch:31900\n",
      "epoch:32000\n",
      "epoch:32100\n",
      "epoch:32200\n",
      "epoch:32300\n",
      "epoch:32400\n",
      "epoch:32500\n",
      "epoch:32600\n",
      "epoch:32700\n",
      "epoch:32800\n",
      "epoch:32900\n",
      "epoch:33000\n",
      "epoch:33100\n",
      "epoch:33200\n",
      "epoch:33300\n",
      "epoch:33400\n",
      "epoch:33500\n",
      "epoch:33600\n",
      "epoch:33700\n",
      "epoch:33800\n",
      "epoch:33900\n",
      "epoch:34000\n",
      "epoch:34100\n",
      "epoch:34200\n",
      "epoch:34300\n",
      "epoch:34400\n",
      "epoch:34500\n",
      "epoch:34600\n",
      "epoch:34700\n",
      "epoch:34800\n",
      "epoch:34900\n",
      "epoch:35000\n",
      "epoch:35100\n",
      "epoch:35200\n",
      "epoch:35300\n",
      "epoch:35400\n",
      "epoch:35500\n",
      "epoch:35600\n",
      "epoch:35700\n",
      "epoch:35800\n",
      "epoch:35900\n",
      "epoch:36000\n",
      "epoch:36100\n",
      "epoch:36200\n",
      "epoch:36300\n",
      "epoch:36400\n",
      "epoch:36500\n",
      "epoch:36600\n",
      "epoch:36700\n",
      "epoch:36800\n",
      "epoch:36900\n",
      "epoch:37000\n",
      "epoch:37100\n",
      "epoch:37200\n",
      "epoch:37300\n",
      "epoch:37400\n",
      "epoch:37500\n",
      "epoch:37600\n",
      "epoch:37700\n",
      "epoch:37800\n",
      "epoch:37900\n",
      "epoch:38000\n",
      "epoch:38100\n",
      "epoch:38200\n",
      "epoch:38300\n",
      "epoch:38400\n",
      "epoch:38500\n",
      "epoch:38600\n",
      "epoch:38700\n",
      "epoch:38800\n",
      "epoch:38900\n",
      "epoch:39000\n",
      "epoch:39100\n",
      "epoch:39200\n",
      "epoch:39300\n",
      "epoch:39400\n",
      "epoch:39500\n",
      "epoch:39600\n",
      "epoch:39700\n",
      "epoch:39800\n",
      "epoch:39900\n",
      "epoch:40000\n",
      "epoch:40100\n",
      "epoch:40200\n",
      "epoch:40300\n",
      "epoch:40400\n",
      "epoch:40500\n",
      "epoch:40600\n",
      "epoch:40700\n",
      "epoch:40800\n",
      "epoch:40900\n",
      "epoch:41000\n",
      "epoch:41100\n",
      "epoch:41200\n",
      "epoch:41300\n",
      "epoch:41400\n",
      "epoch:41500\n",
      "epoch:41600\n",
      "epoch:41700\n",
      "epoch:41800\n",
      "epoch:41900\n",
      "epoch:42000\n",
      "epoch:42100\n",
      "epoch:42200\n",
      "epoch:42300\n",
      "epoch:42400\n",
      "epoch:42500\n",
      "epoch:42600\n",
      "epoch:42700\n",
      "epoch:42800\n",
      "epoch:42900\n",
      "epoch:43000\n",
      "epoch:43100\n",
      "epoch:43200\n",
      "epoch:43300\n",
      "epoch:43400\n",
      "epoch:43500\n",
      "epoch:43600\n",
      "epoch:43700\n",
      "epoch:43800\n",
      "epoch:43900\n",
      "epoch:44000\n",
      "epoch:44100\n",
      "epoch:44200\n",
      "epoch:44300\n",
      "epoch:44400\n",
      "epoch:44500\n",
      "epoch:44600\n",
      "epoch:44700\n",
      "epoch:44800\n",
      "epoch:44900\n",
      "epoch:45000\n",
      "epoch:45100\n",
      "epoch:45200\n",
      "epoch:45300\n",
      "epoch:45400\n",
      "epoch:45500\n",
      "epoch:45600\n",
      "epoch:45700\n",
      "epoch:45800\n",
      "epoch:45900\n",
      "epoch:46000\n",
      "epoch:46100\n",
      "epoch:46200\n",
      "epoch:46300\n",
      "epoch:46400\n",
      "epoch:46500\n",
      "epoch:46600\n",
      "epoch:46700\n",
      "epoch:46800\n",
      "epoch:46900\n",
      "epoch:47000\n",
      "epoch:47100\n",
      "epoch:47200\n",
      "epoch:47300\n",
      "epoch:47400\n",
      "epoch:47500\n"
     ]
    }
   ],
   "source": [
    "gan = GAN()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
