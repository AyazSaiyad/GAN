{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cramer GAN implementation by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# utility func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save metrics\n",
    "def save_metrics(metrics, epoch=None):\n",
    "    # make directory if there is not\n",
    "    path = \"metrics\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # save metrics\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"dis_loss\"], label=\"discriminative loss\", color=\"b\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"dloss\" + str(epoch) + \".png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"gen_loss\"], label=\"generative loss\", color=\"r\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"g_loss\" + str(epoch) + \".png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(metrics[\"gen_loss\"], label=\"generative loss\", color=\"r\")\n",
    "    plt.plot(metrics[\"dis_loss\"], label=\"discriminative loss\", color=\"b\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path, \"both_loss\" + str(epoch) + \".png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot images\n",
    "def save_imgs(images, plot_dim=(5,12), size=(12,5), epoch=None):\n",
    "    # make directory if there is not\n",
    "    path = \"generated_figures\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    num_examples = plot_dim[0]*plot_dim[1]\n",
    "    num_examples = 60\n",
    "    fig = plt.figure(figsize=size)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        plt.subplot(plot_dim[0], plot_dim[1], i+1)\n",
    "        img = images[i, :]\n",
    "        img = img.reshape((96, 96, 3))\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.savefig(os.path.join(path, str(epoch) + \".png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator_UNet:\n",
    "    def __init__(self, name=\"test\"):\n",
    "        self.reuse = False\n",
    "        self.initializer = tf.contrib.layers.xavier_initializer()\n",
    "        self.X_dim = 96*96*3\n",
    "        self.z_dim = 100\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, inputs, gf_dim=64):\n",
    "        def leaky_relu(x, leak=0.2, name='outputs'):\n",
    "            return tf.maximum(x, x * leak, name=name)\n",
    "\n",
    "        def instance_norm(input, name):\n",
    "            with tf.variable_scope(name):\n",
    "                depth = input.get_shape()[3]\n",
    "                scale = tf.get_variable(\"scale\", [depth], initializer=tf.random_normal_initializer(1.0, 0.02, dtype=tf.float32))\n",
    "                offset = tf.get_variable(\"offset\", [depth], initializer=tf.constant_initializer(0.0))\n",
    "                mean, variance = tf.nn.moments(input, axes=[1,2], keep_dims=True)\n",
    "                epsilon = 1e-5\n",
    "                inv = tf.rsqrt(variance + epsilon)\n",
    "                normalized = (input-mean)*inv\n",
    "            return scale*normalized + offset\n",
    "\n",
    "        with tf.variable_scope('g_' + self.name, reuse=self.reuse):\n",
    "            # 256x256x3\n",
    "            print(\"called... name of g:\" + str(self.name))\n",
    "            inputs = tf.reshape(inputs, [-1, self.z_dim])\n",
    "            fc1 = inputs\n",
    "            fc2 = tf.layers.dense(fc1, 96*96*3)\n",
    "            fc2 = tf.reshape(fc2, [-1, 96,96,3])\n",
    "\n",
    "\n",
    "            e0 = tf.layers.conv2d(fc2, gf_dim, [3,3], [2,2], padding=\"SAME\") # 48x48\n",
    "            e1 = leaky_relu(e0)\n",
    "            e1 = tf.layers.conv2d(e1, gf_dim*2, [3,3], [2,2], padding=\"SAME\")\n",
    "            #e1 = instance_norm(e1, name=\"e1\")\n",
    "            # 24x24\n",
    "\n",
    "            e2 = leaky_relu(e1)\n",
    "            e2 = tf.layers.conv2d(e2, gf_dim*4, [3,3], [2,2], padding=\"SAME\")\n",
    "            #e2 = instance_norm(e2, name=\"e2\")\n",
    "            # 12x12\n",
    "\n",
    "            e3 = leaky_relu(e2)\n",
    "            e3 = tf.layers.conv2d(e3, gf_dim*8, [3,3], [2,2], padding=\"SAME\")\n",
    "            #e3 = instance_norm(e3, name=\"e3\")\n",
    "            # 6x6\n",
    "\n",
    "            e4 = leaky_relu(e3)\n",
    "            e4 = tf.layers.conv2d(e4, gf_dim*8, [3,3], [2,2], padding=\"SAME\")\n",
    "            #e4 = instance_norm(e4, name=\"e4\")\n",
    "            # 3x3\n",
    "\n",
    "            # \n",
    "            d0 = tf.nn.relu(e4)\n",
    "            d0 = tf.layers.conv2d_transpose(d0, gf_dim*8, [3,3], [2,2], padding=\"SAME\") \n",
    "            #d0 = instance_norm(d0, name=\"d0\")\n",
    "            #d0 = tf.nn.dropout(d0, 0.5)\n",
    "            d0 = tf.concat([d0, e3], 3)\n",
    "            # 6x6\n",
    "            \n",
    "            d1 = tf.nn.relu(d0)\n",
    "            d1 = tf.layers.conv2d_transpose(d1, gf_dim*4, [3,3], [2,2], padding=\"SAME\") \n",
    "            #d1 = instance_norm(d1, name=\"d1\")\n",
    "            #d1 = tf.nn.dropout(d1, 0.5)\n",
    "            d1 = tf.concat([d1, e2], 3)\n",
    "            # 12x12 \n",
    "\n",
    "            d2 = tf.nn.relu(d1)\n",
    "            d2 = tf.layers.conv2d_transpose(d2, gf_dim*2, [3,3], [2,2], padding=\"SAME\") \n",
    "            #d2 = instance_norm(d2, name=\"d2\")\n",
    "            #d2 = tf.nn.dropout(d2, 0.5)\n",
    "            d2 = tf.concat([d2, e1], 3)\n",
    "            # 24x24\n",
    "\n",
    "            d3 = tf.nn.relu(d2)\n",
    "            d3 = tf.layers.conv2d_transpose(d3, gf_dim, [3,3], [2,2], padding=\"SAME\") \n",
    "            #d3 = instance_norm(d3, name=\"d3\")\n",
    "            d3 = tf.concat([d3, e0], 3)\n",
    "            # 48x48\n",
    "\n",
    "            d7 = tf.nn.relu(d3)\n",
    "            d7 = tf.layers.conv2d_transpose(d7, 3, [3,3], [2,2], padding=\"SAME\") \n",
    "            d7 = tf.nn.tanh(d7)\n",
    "            # 96x96x3\n",
    "\n",
    "            conv4 = tf.reshape(d7, [-1, 96*96*3])\n",
    "            outputs = conv4\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='g')\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.reuse = False\n",
    "        self.X_dim = 96*96*3\n",
    "        self.initializer = tf.contrib.layers.xavier_initializer()\n",
    "        self.s_size = 6\n",
    "\n",
    "    def __call__(self, inputs, training=False, name=''):\n",
    "        def leaky_relu(x, leak=0.2, name='outputs'):\n",
    "            return tf.maximum(x, x * leak, name=name)\n",
    "\n",
    "        with tf.name_scope('d' + name), tf.variable_scope('d', reuse=self.reuse):\n",
    "            x = tf.reshape(inputs, [-1, 96, 96, 3])\n",
    "            conv1 = tf.layers.conv2d(x, 64, [4,4], [2,2])\n",
    "            conv1 = leaky_relu(conv1)\n",
    "            conv2 = tf.layers.conv2d(conv1, 128, [4,4], [2,2])\n",
    "            conv2 = leaky_relu(conv2)\n",
    "            conv3 = tf.layers.conv2d(conv2, 256, [4,4], [2,2])\n",
    "            conv3 = leaky_relu(conv3)\n",
    "            conv4 = tf.layers.conv2d(conv3, 512, [4,4], [2,2])\n",
    "            conv4 = leaky_relu(conv4)\n",
    "            conv4 = tf.contrib.layers.flatten(conv4)\n",
    "            fc1 = tf.layers.dense(conv4, 512)\n",
    "            fc1 = leaky_relu(fc1)\n",
    "            fc2 = tf.layers.dense(fc1, 256)\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='d')\n",
    "        return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f(x) = ||h(x) - h(x_)|| - ||h(x)||\n",
    "class Critic(object):\n",
    "    def __init__(self, h):\n",
    "        self.h = h\n",
    "    def __call__(self, x, x_):\n",
    "        return tf.norm(self.h(x) - self.h(x_), axis=1) - tf.norm(self.h(x), axis=1)\n",
    "\n",
    "# f(x) = ||h(x) - h(x_)||\n",
    "class calc_Norm(object):\n",
    "    def __init__(self, h):\n",
    "        self.h = h\n",
    "    def __call__(self, x, x_):\n",
    "        return tf.norm(self.h(x) -self.h(x_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.batch_size = batch_size\n",
    "        self.g = Generator_UNet()\n",
    "        self.d = Discriminator()\n",
    "        self.z_dim = 100\n",
    "        self.critic = Critic(self.d)\n",
    "        self.calcNorm = calc_Norm(self.d)\n",
    "\n",
    "    def loss(self, Xr, z1, z2):\n",
    "        Xg = self.g(z1)\n",
    "        Xg2 = self.g(z2)\n",
    "        Xg = tf.reshape(Xg, [batch_size, -1])\n",
    "        Xg2 = tf.reshape(Xg2, [batch_size, -1])\n",
    "\n",
    "        L_surrogate = tf.reduce_mean(self.critic(Xr, Xg2) - self.critic(Xg, Xg2))\n",
    "        L_critic = -L_surrogate\n",
    "\n",
    "        epsilon = tf.random_uniform([], 0.0, 1.0) # 怪しい\n",
    "\n",
    "        Xhat = epsilon * Xr + (1-epsilon) * Xg\n",
    "        dhat = self.critic(Xhat, Xg2)\n",
    "\n",
    "        ddx = tf.gradients(dhat, Xhat)[0]\n",
    "        ddx = tf.norm(ddx, axis=1)\n",
    "        ddx = tf.reduce_mean(tf.square(ddx - 1.0) * 10)\n",
    "\n",
    "        L_critic = L_critic + ddx\n",
    "        # add each losses to collection\n",
    "        tf.add_to_collection(\n",
    "            'g_losses',\n",
    "            L_surrogate)\n",
    "\n",
    "        tf.add_to_collection(\n",
    "            'd_losses',\n",
    "            L_critic\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            self.g: tf.add_n(tf.get_collection('g_losses'), name='total_g_loss'),\n",
    "            self.d: tf.add_n(tf.get_collection('d_losses'), name='total_d_loss'),\n",
    "        }\n",
    "\n",
    "    def d_train(self, losses, learning_rate=1e-4, beta1=0.5, beta2=0.9):\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2)\n",
    "        d_opt_op = d_opt.minimize(losses[self.d], var_list=self.d.variables)\n",
    "        return d_opt_op\n",
    "\n",
    "    def g_train(self, losses, learning_rate=1e-4, beta1=0.5, beta2=0.9):\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2)\n",
    "        g_opt_op = g_opt.minimize(losses[self.g], var_list=self.g.variables)\n",
    "        return g_opt_op\n",
    "\n",
    "    def sample_images(self, row=5, col=12, inputs=None, epoch=None):\n",
    "        images = self.g(inputs)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load(\"irasutoya_face_1813x96x96x3_jpg.npy\")\n",
    "X_train = X_train/255\n",
    "X_dim = 96*96*3\n",
    "z_dim = 100\n",
    "batch_size = 32\n",
    "epochs = 500000\n",
    "display_epoch = 100\n",
    "param_save_epoch = 10000\n",
    "loss = {\"dis_loss\":[], \"gen_loss\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called... name of g:test\n",
      "called... name of g:test\n",
      "called... name of g:test\n",
      "saving images\n",
      "epoch:0\n",
      "epoch:1\n",
      "epoch:2\n",
      "epoch:3\n",
      "epoch:4\n",
      "epoch:5\n",
      "epoch:6\n",
      "epoch:7\n",
      "epoch:8\n",
      "epoch:9\n",
      "epoch:10\n",
      "epoch:11\n",
      "epoch:12\n",
      "epoch:13\n",
      "epoch:14\n",
      "epoch:15\n",
      "epoch:16\n",
      "epoch:17\n",
      "epoch:18\n",
      "epoch:19\n",
      "epoch:20\n",
      "epoch:21\n",
      "epoch:22\n",
      "epoch:23\n",
      "epoch:24\n",
      "epoch:25\n",
      "epoch:26\n",
      "epoch:27\n",
      "epoch:28\n",
      "epoch:29\n",
      "epoch:30\n",
      "epoch:31\n",
      "epoch:32\n",
      "epoch:33\n",
      "epoch:34\n",
      "epoch:35\n",
      "epoch:36\n",
      "epoch:37\n",
      "epoch:38\n",
      "epoch:39\n",
      "epoch:40\n",
      "epoch:41\n",
      "epoch:42\n",
      "epoch:43\n",
      "epoch:44\n",
      "epoch:45\n",
      "epoch:46\n",
      "epoch:47\n",
      "epoch:48\n",
      "epoch:49\n",
      "epoch:50\n",
      "epoch:51\n",
      "epoch:52\n",
      "epoch:53\n",
      "epoch:54\n",
      "epoch:55\n",
      "epoch:56\n",
      "epoch:57\n",
      "epoch:58\n",
      "epoch:59\n",
      "epoch:60\n",
      "epoch:61\n",
      "epoch:62\n",
      "epoch:63\n",
      "epoch:64\n",
      "epoch:65\n",
      "epoch:66\n",
      "epoch:67\n",
      "epoch:68\n",
      "epoch:69\n",
      "epoch:70\n",
      "epoch:71\n",
      "epoch:72\n",
      "epoch:73\n",
      "epoch:74\n",
      "epoch:75\n",
      "epoch:76\n",
      "epoch:77\n",
      "epoch:78\n",
      "epoch:79\n",
      "epoch:80\n",
      "epoch:81\n",
      "epoch:82\n",
      "epoch:83\n",
      "epoch:84\n",
      "epoch:85\n",
      "epoch:86\n",
      "epoch:87\n",
      "epoch:88\n",
      "epoch:89\n",
      "epoch:90\n",
      "epoch:91\n",
      "epoch:92\n",
      "epoch:93\n",
      "epoch:94\n",
      "epoch:95\n",
      "epoch:96\n",
      "epoch:97\n",
      "epoch:98\n",
      "epoch:99\n",
      "called... name of g:test\n",
      "saving images\n",
      "epoch:100\n",
      "epoch:101\n",
      "epoch:102\n",
      "epoch:103\n",
      "epoch:104\n",
      "epoch:105\n",
      "epoch:106\n",
      "epoch:107\n",
      "epoch:108\n",
      "epoch:109\n",
      "epoch:110\n",
      "epoch:111\n",
      "epoch:112\n",
      "epoch:113\n",
      "epoch:114\n",
      "epoch:115\n",
      "epoch:116\n",
      "epoch:117\n",
      "epoch:118\n",
      "epoch:119\n",
      "epoch:120\n",
      "epoch:121\n",
      "epoch:122\n",
      "epoch:123\n",
      "epoch:124\n",
      "epoch:125\n",
      "epoch:126\n",
      "epoch:127\n",
      "epoch:128\n",
      "epoch:129\n",
      "epoch:130\n",
      "epoch:131\n",
      "epoch:132\n",
      "epoch:133\n",
      "epoch:134\n",
      "epoch:135\n",
      "epoch:136\n",
      "epoch:137\n",
      "epoch:138\n",
      "epoch:139\n",
      "epoch:140\n",
      "epoch:141\n",
      "epoch:142\n",
      "epoch:143\n",
      "epoch:144\n",
      "epoch:145\n",
      "epoch:146\n",
      "epoch:147\n",
      "epoch:148\n",
      "epoch:149\n",
      "epoch:150\n",
      "epoch:151\n",
      "epoch:152\n",
      "epoch:153\n",
      "epoch:154\n",
      "epoch:155\n",
      "epoch:156\n",
      "epoch:157\n",
      "epoch:158\n",
      "epoch:159\n",
      "epoch:160\n",
      "epoch:161\n",
      "epoch:162\n",
      "epoch:163\n",
      "epoch:164\n",
      "epoch:165\n",
      "epoch:166\n",
      "epoch:167\n",
      "epoch:168\n",
      "epoch:169\n",
      "epoch:170\n",
      "epoch:171\n",
      "epoch:172\n",
      "epoch:173\n",
      "epoch:174\n",
      "epoch:175\n",
      "epoch:176\n",
      "epoch:177\n",
      "epoch:178\n",
      "epoch:179\n",
      "epoch:180\n",
      "epoch:181\n",
      "epoch:182\n",
      "epoch:183\n",
      "epoch:184\n",
      "epoch:185\n",
      "epoch:186\n",
      "epoch:187\n",
      "epoch:188\n",
      "epoch:189\n",
      "epoch:190\n",
      "epoch:191\n",
      "epoch:192\n",
      "epoch:193\n",
      "epoch:194\n",
      "epoch:195\n",
      "epoch:196\n",
      "epoch:197\n",
      "epoch:198\n",
      "epoch:199\n",
      "called... name of g:test\n",
      "saving images\n",
      "epoch:200\n",
      "epoch:201\n",
      "epoch:202\n",
      "epoch:203\n",
      "epoch:204\n",
      "epoch:205\n",
      "epoch:206\n",
      "epoch:207\n",
      "epoch:208\n",
      "epoch:209\n",
      "epoch:210\n",
      "epoch:211\n",
      "epoch:212\n",
      "epoch:213\n",
      "epoch:214\n",
      "epoch:215\n",
      "epoch:216\n",
      "epoch:217\n",
      "epoch:218\n",
      "epoch:219\n",
      "epoch:220\n",
      "epoch:221\n",
      "epoch:222\n",
      "epoch:223\n",
      "epoch:224\n",
      "epoch:225\n",
      "epoch:226\n",
      "epoch:227\n"
     ]
    }
   ],
   "source": [
    "p_noise = tf.placeholder(tf.float32, [None, z_dim])\n",
    "noise_check = np.random.uniform(-1, 1, size=[60, z_dim]).astype(np.float32)\n",
    "\n",
    "Xr = tf.placeholder(tf.float32, [None, X_dim])\n",
    "Z1 = tf.placeholder(tf.float32, [None, z_dim])\n",
    "Z2 = tf.placeholder(tf.float32, [None, z_dim])\n",
    "\n",
    "gan = GAN()\n",
    "\n",
    "losses = gan.loss(Xr=Xr, z1=Z1, z2=Z2)\n",
    "d_train_op = gan.d_train(losses)\n",
    "g_train_op = gan.g_train(losses)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#%debug\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % display_epoch == 0:\n",
    "            imgs_gen = gan.sample_images(inputs=noise_check).eval()\n",
    "            print(\"saving images\")\n",
    "            save_imgs(imgs_gen, epoch=epoch)\n",
    "\n",
    "        for _ in range(5):\n",
    "            # 訓練データを抜粋\n",
    "            rand_index = np.random.randint(0, X_train.shape[0], size=batch_size)\n",
    "            X_mb = X_train[rand_index, :].astype(np.float32)\n",
    "            X_mb = X_mb.reshape([-1, X_dim])\n",
    "\n",
    "            z1 = np.random.uniform(-1, 1, size=[batch_size, z_dim])\n",
    "            z2 = np.random.uniform(-1, 1, size=[batch_size, z_dim])\n",
    "\n",
    "            _, d_loss_value = sess.run([d_train_op, losses[gan.d]], feed_dict={Xr: X_mb, Z1:z1, Z2:z2})\n",
    "\n",
    "\n",
    "        # 訓練データを抜粋\n",
    "        rand_index = np.random.randint(0, X_train.shape[0], size=batch_size)\n",
    "        X_mb = X_train[rand_index, :].astype(np.float32)\n",
    "        X_mb = X_mb.reshape([-1, X_dim])\n",
    "        z1 = np.random.uniform(-1, 1, size=[batch_size, z_dim])\n",
    "        z2 = np.random.uniform(-1, 1, size=[batch_size, z_dim])\n",
    "        _, g_loss_value, = sess.run([g_train_op, losses[gan.g]], feed_dict={Xr: X_mb, Z1:z1, Z2:z2})\n",
    "\n",
    "        # 結果をappend\n",
    "        loss[\"dis_loss\"].append(d_loss_value)\n",
    "        loss[\"gen_loss\"].append(g_loss_value)\n",
    "        print(\"epoch:\" + str(epoch))\n",
    "        # グラフの描画（余裕があったら）\n",
    "        if epoch % display_epoch == 0:\n",
    "            save_metrics(loss, epoch)\n",
    "\n",
    "        if epoch % param_save_epoch == 0:\n",
    "            path = \"model\"\n",
    "            if not os.path.isdir(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "            saver.save(sess, \"./model/dcgan_model\" + str(epoch) + \".ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
